{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aLfmghypEaUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites and imports"
      ],
      "metadata": {
        "id": "3MmOims1B-hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y lz4\n",
        "!pip install matplotlib\n",
        "!pip install gdown\n",
        "!pip install paddleocr\n",
        "!pip install paddlepaddle\n",
        "!pip install editdistance\n",
        "!pip install yolov5\n",
        "!pip install ultralytics\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPLX0ypHBkvy",
        "outputId": "3f941284-0db3-4bb5-a147-cebfb19195d9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  lz4\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 90.0 kB of archives.\n",
            "After this operation, 236 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 lz4 amd64 1.9.3-2build2 [90.0 kB]\n",
            "Fetched 90.0 kB in 1s (128 kB/s)\n",
            "Selecting previously unselected package lz4.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../lz4_1.9.3-2build2_amd64.deb ...\n",
            "Unpacking lz4 (1.9.3-2build2) ...\n",
            "Setting up lz4 (1.9.3-2build2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.9)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting paddleocr\n",
            "  Downloading paddleocr-3.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting paddlex>=3.1.0 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading paddlex-3.1.3-py3-none-any.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6 in /usr/local/lib/python3.11/dist-packages (from paddleocr) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from paddleocr) (4.14.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.2.0)\n",
            "Collecting colorlog (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.18.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (24.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (11.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.16.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.11.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.32.3)\n",
            "Collecting ruamel.yaml (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ujson (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.8.1)\n",
            "Collecting ftfy (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting GPUtil>=1.4 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.6)\n",
            "Collecting opencv-contrib-python==4.10.0.84 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pypdfium2>=4 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2024.11.6)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.4.1)\n",
            "Requirement already satisfied: langchain>=0.2 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.26)\n",
            "Collecting langchain-community>=0.2 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.68)\n",
            "Collecting langchain-openai>=0.1 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.4.0)\n",
            "Requirement already satisfied: openai>=1.63 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.93.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.5)\n",
            "Collecting premailer (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading premailer-3.10.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyclipper (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.6.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.1.1)\n",
            "Requirement already satisfied: tokenizers>=0.19 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.21.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.13.4)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.4.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.33)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.7.9)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.1.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.0.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.0)\n",
            "Collecting cssselect (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting cssutils (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading cssutils-2.11.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.5.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.2.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from cssutils->premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (10.7.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading paddleocr-3.1.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paddlex-3.1.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading cssutils-2.11.1-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=4d3b9e2ae60fa475010ba6fcd515673f4c90e11607eff8084ad62ebe399f63bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: pyclipper, GPUtil, ujson, ruamel.yaml.clib, python-dotenv, pypdfium2, opencv-contrib-python, mypy-extensions, marshmallow, httpx-sse, ftfy, cssutils, cssselect, colorlog, typing-inspect, ruamel.yaml, premailer, pydantic-settings, paddlex, dataclasses-json, langchain-openai, langchain-community, paddleocr\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.11.0.86\n",
            "    Uninstalling opencv-contrib-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
            "Successfully installed GPUtil-1.4.0 colorlog-6.9.0 cssselect-1.3.0 cssutils-2.11.1 dataclasses-json-0.6.7 ftfy-6.3.1 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-openai-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 opencv-contrib-python-4.10.0.84 paddleocr-3.1.0 paddlex-3.1.3 premailer-3.10.0 pyclipper-1.3.0.post6 pydantic-settings-2.10.1 pypdfium2-4.30.1 python-dotenv-1.1.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 typing-inspect-0.9.0 ujson-5.10.0\n",
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-3.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (5.29.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (11.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Collecting opt_einsum==3.3.0 (from paddlepaddle)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (3.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (4.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->paddlepaddle) (1.3.1)\n",
            "Downloading paddlepaddle-3.1.0-cp311-cp311-manylinux1_x86_64.whl (195.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.0/195.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opt_einsum, paddlepaddle\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "Successfully installed opt_einsum-3.3.0 paddlepaddle-3.1.0\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting yolov5\n",
            "  Downloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from yolov5) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from yolov5) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from yolov5) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (4.11.0.86)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from yolov5) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from yolov5) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from yolov5) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (1.15.3)\n",
            "Collecting thop>=0.1.1 (from yolov5)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from yolov5) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from yolov5) (4.67.1)\n",
            "Collecting ultralytics>=8.0.100 (from yolov5)\n",
            "  Downloading ultralytics-8.3.165-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (2.18.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from yolov5) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from yolov5) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.11/dist-packages (from yolov5) (75.2.0)\n",
            "Collecting fire (from yolov5)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3>=1.19.1 (from yolov5)\n",
            "  Downloading boto3-1.39.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sahi>=0.11.10 (from yolov5)\n",
            "  Downloading sahi-0.11.30-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting huggingface-hub<0.25.0,>=0.12.0 (from yolov5)\n",
            "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting roboflow>=0.2.29 (from yolov5)\n",
            "  Downloading roboflow-1.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting botocore<1.40.0,>=1.39.4 (from boto3>=1.19.1->yolov5)\n",
            "  Downloading botocore-1.39.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.19.1->yolov5)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.19.1->yolov5)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->yolov5) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (4.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->yolov5) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->yolov5) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->yolov5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->yolov5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->yolov5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->yolov5) (2025.7.9)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.23.0->yolov5)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow>=0.2.29->yolov5)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pillow-heif<2 (from roboflow>=0.2.29->yolov5)\n",
            "  Downloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow>=0.2.29->yolov5)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow>=0.2.29->yolov5) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow>=0.2.29->yolov5) (1.17.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n",
            "Collecting filetype (from roboflow>=0.2.29->yolov5)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sahi>=0.11.10->yolov5) (8.2.1)\n",
            "Collecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n",
            "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from sahi>=0.11.10->yolov5) (2.1.1)\n",
            "Collecting terminaltables (from sahi>=0.11.10->yolov5)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->yolov5) (3.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.0->yolov5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->yolov5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->yolov5) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.0.100->yolov5)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->yolov5) (3.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (3.0.2)\n",
            "Downloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.5/953.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.39.4-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading roboflow-1.2.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sahi-0.11.30-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.165-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.4-py3-none-any.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=654922159c855466d57a1adaca417e72e2b1f21b9687e4d32077becf5f3e49f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: pillow-avif-plugin, filetype, terminaltables, pybboxes, pillow-heif, opencv-python-headless, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, idna, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, sahi, s3transfer, nvidia-cusolver-cu12, huggingface-hub, roboflow, boto3, ultralytics-thop, thop, ultralytics, yolov5\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.2\n",
            "    Uninstalling huggingface-hub-0.33.2:\n",
            "      Successfully uninstalled huggingface-hub-0.33.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.53.1 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 0.24.7 which is incompatible.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.24.7 which is incompatible.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.24.7 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.24.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.39.4 botocore-1.39.4 filetype-1.2.0 fire-0.7.0 huggingface-hub-0.24.7 idna-3.7 jmespath-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-headless-4.10.0.84 pillow-avif-plugin-1.5.2 pillow-heif-1.0.0 pybboxes-0.1.6 roboflow-1.2.1 s3transfer-0.13.0 sahi-0.11.30 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.3.165 ultralytics-thop-2.0.14 yolov5-7.0.14\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.165)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import warnings\n",
        "import editdistance\n",
        "import yolov5\n",
        "from paddleocr import PaddleOCR\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import shutil\n",
        "from matplotlib import font_manager"
      ],
      "metadata": {
        "id": "NcQyDTySBnRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05838b10-f3f8-4442-ff90-3fbbe342cab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:32:13.114694Z",
          "iopub.status.busy": "2025-07-03T09:32:13.114497Z",
          "iopub.status.idle": "2025-07-03T09:32:13.118083Z",
          "shell.execute_reply": "2025-07-03T09:32:13.117584Z",
          "shell.execute_reply.started": "2025-07-03T09:32:13.114669Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdk0fKs8BEv4",
        "outputId": "bc3bea58-f885-449a-ab7c-c385f1bbf0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE_DIR: /content\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = os.getcwd()\n",
        "print(f\"BASE_DIR: {BASE_DIR}\")\n",
        "FONT_PATH = f\"{BASE_DIR}/NotoSansTC-VariableFont_wght.ttf\"\n",
        "YOLO_MODEL = \"yolov5s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egbAiWbgfGmf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Download and decompress dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-03T09:28:52.659812Z",
          "iopub.status.busy": "2025-07-03T09:28:52.659278Z",
          "iopub.status.idle": "2025-07-03T09:32:13.113348Z",
          "shell.execute_reply": "2025-07-03T09:32:13.112511Z",
          "shell.execute_reply.started": "2025-07-03T09:28:52.659790Z"
        },
        "id": "rL9GgzsioypX",
        "outputId": "d907347f-1df1-42a3-c19d-e4ba291bb366",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1_u2OTTt2l81jIhS0PTP6ekpfkchfHdoZ NotoSansTC-VariableFont_wght.ttf\n",
            "Processing file 1isPOnFzFXBd35Rk_bWiVjpVcCVssO_0q ccpd_train.tar.lz4\n",
            "Processing file 1SIeRKfz7JvpXPG-VvDxhs1ry0N4m-zyy ccpd_val.tar.lz4\n",
            "Processing file 1Smvr3gTDAed6K6mW5yZDSx--bsEU6Gat NotoSansSC-VariableFont_wght.ttf\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_u2OTTt2l81jIhS0PTP6ekpfkchfHdoZ\n",
            "To: /content/CCPD/NotoSansTC-VariableFont_wght.ttf\n",
            "100% 11.9M/11.9M [00:00<00:00, 48.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1isPOnFzFXBd35Rk_bWiVjpVcCVssO_0q\n",
            "From (redirected): https://drive.google.com/uc?id=1isPOnFzFXBd35Rk_bWiVjpVcCVssO_0q&confirm=t&uuid=e69e52cf-1c46-4015-bb2e-e19973d0c46f\n",
            "To: /content/CCPD/ccpd_train.tar.lz4\n",
            "100% 6.57G/6.57G [01:39<00:00, 66.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SIeRKfz7JvpXPG-VvDxhs1ry0N4m-zyy\n",
            "From (redirected): https://drive.google.com/uc?id=1SIeRKfz7JvpXPG-VvDxhs1ry0N4m-zyy&confirm=t&uuid=083738a8-0fdb-425e-959d-06b68bc8797e\n",
            "To: /content/CCPD/ccpd_val.tar.lz4\n",
            "100% 6.56G/6.56G [01:19<00:00, 82.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Smvr3gTDAed6K6mW5yZDSx--bsEU6Gat\n",
            "To: /content/CCPD/NotoSansSC-VariableFont_wght.ttf\n",
            "100% 17.8M/17.8M [00:00<00:00, 107MB/s] \n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/19lbkl8seJ56jQuj7hVq5sv5Dj-G4RvM-?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCYqvcvtEy3t"
      },
      "source": [
        "Decompress split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj6g6JbgD66v",
        "outputId": "6e5c7f43-0d67-417c-9e17-5b2467a735b9",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CCPD/ccpd_train.tar.lz4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['tar', '--use-compress-program=lz4', '-xvf', '/content/CCPD/ccpd_val.tar.lz4', '-C', '/content/CCPD2019'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "if not os.path.exists(f\"{BASE_DIR}/CCPD2019\"):\n",
        "    os.makedirs(f\"{BASE_DIR}/CCPD2019\")\n",
        "print(f\"{BASE_DIR}/CCPD/ccpd_train.tar.lz4\")\n",
        "subprocess.run([\"tar\", \"--use-compress-program=lz4\", \"-xvf\", f\"{BASE_DIR}/CCPD/ccpd_train.tar.lz4\", \"-C\", f\"{BASE_DIR}/CCPD2019\"])\n",
        "subprocess.run([\"tar\", \"--use-compress-program=lz4\", \"-xvf\", f\"{BASE_DIR}/CCPD/ccpd_val.tar.lz4\", \"-C\", f\"{BASE_DIR}/CCPD2019\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set parameters"
      ],
      "metadata": {
        "id": "3W5edcHDkYmZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T10:38:49.231578Z",
          "iopub.status.busy": "2025-07-03T10:38:49.231032Z",
          "iopub.status.idle": "2025-07-03T10:38:49.234534Z",
          "shell.execute_reply": "2025-07-03T10:38:49.233949Z",
          "shell.execute_reply.started": "2025-07-03T10:38:49.231561Z"
        },
        "id": "9_9PP9p3nCsL"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 10000\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "IOU_THRESHOLD = 0.5\n",
        "LEARNING_RATE = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2tgLoHCxYtU"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:33:22.812756Z",
          "iopub.status.busy": "2025-07-03T09:33:22.812370Z",
          "iopub.status.idle": "2025-07-03T09:33:22.815989Z",
          "shell.execute_reply": "2025-07-03T09:33:22.815496Z",
          "shell.execute_reply.started": "2025-07-03T09:33:22.812731Z"
        },
        "id": "0wMp-d5sE88E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9997344-8f3c-4c13-db36-64b3da159f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_DATASET: /content/CCPD2019/ccpd_train\n",
            "VAL_DATASET: /content/CCPD2019/ccpd_val\n"
          ]
        }
      ],
      "source": [
        "TRAIN_DATASET = f'{BASE_DIR}/CCPD2019/ccpd_train'\n",
        "VAL_DATASET = f'{BASE_DIR}/CCPD2019/ccpd_val'\n",
        "print(f\"TRAIN_DATASET: {TRAIN_DATASET}\")\n",
        "print(f\"VAL_DATASET: {VAL_DATASET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:34:01.649424Z",
          "iopub.status.busy": "2025-07-03T09:34:01.648883Z",
          "iopub.status.idle": "2025-07-03T09:34:01.658437Z",
          "shell.execute_reply": "2025-07-03T09:34:01.657844Z",
          "shell.execute_reply.started": "2025-07-03T09:34:01.649404Z"
        },
        "id": "gmk8YrJ-uANn"
      },
      "outputs": [],
      "source": [
        "FILENAME_SPLITTER = '&'\n",
        "\n",
        "class CCPDDataset(Dataset):\n",
        "    def __init__(self, data_dir, split_file=None, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        all_image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "\n",
        "        print(f\"Found {len(all_image_files)} image files in the data directory.\")\n",
        "\n",
        "        # If a split file is provided, filter the image files\n",
        "        if split_file:\n",
        "            if not os.path.exists(split_file):\n",
        "                print(f\"Warning: Split file not found at {split_file}\")\n",
        "                self.image_files = [] # No files if split file is missing\n",
        "            else:\n",
        "                print(\"Splitting\")\n",
        "                with open(split_file, 'r') as f:\n",
        "                    # Read filenames from the split file, removing leading/trailing whitespace\n",
        "                    # and removing 'ccpd_blur/' prefix\n",
        "                    split_filenames = [line.strip().replace('ccpd_base/', '') for line in f if line.strip()]\n",
        "\n",
        "\n",
        "                # Filter image files to include only those in the split file\n",
        "                self.image_files = [f for f in all_image_files if os.path.basename(f) in split_filenames]\n",
        "                print(f\"Found {len(self.image_files)} image files matching the split file.\")\n",
        "        else:\n",
        "            self.image_files = all_image_files\n",
        "            print(\"No split file provided, using all image files.\")\n",
        "\n",
        "\n",
        "        # Define the character mappings\n",
        "        self.provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
        "        self.alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
        "                          'X', 'Y', 'Z', 'O']\n",
        "        self.ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "                    'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        bbox, char_indices = self._extract_annotations_from_filename(os.path.basename(img_path))\n",
        "\n",
        "        # Convert character indices to the license plate string\n",
        "        license_plate_string = self._indices_to_string(char_indices)\n",
        "\n",
        "        img_width, img_height = image.size\n",
        "\n",
        "        # Apply transforms to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image) # Make sure this line is present and indented correctly\n",
        "\n",
        "        bbox = [bbox[0] / img_width, bbox[1] / img_height, bbox[2] / img_width, bbox[3] / img_height]\n",
        "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
        "\n",
        "        return image, bbox, license_plate_string, torch.tensor(char_indices, dtype=torch.long) # Optionally return indices as well\n",
        "    \"\"\"\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        bbox, char_indices = self._extract_annotations_from_filename(os.path.basename(img_path))\n",
        "\n",
        "        license_plate_string = self._indices_to_string(char_indices)\n",
        "\n",
        "        # convert PIL to numpy\n",
        "        image_np = np.array(image)\n",
        "\n",
        "        img_h, img_w = image_np.shape[:2]\n",
        "\n",
        "        # Albumentations expects absolute pixel coordinates\n",
        "        bbox_abs = [\n",
        "            bbox[0],\n",
        "            bbox[1],\n",
        "            bbox[2],\n",
        "            bbox[3]\n",
        "        ]\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(\n",
        "                image=image_np,\n",
        "                bboxes=[bbox_abs],   # ABSOLUTE pixel coords\n",
        "                class_labels=[0]\n",
        "            )\n",
        "            image = transformed['image']\n",
        "\n",
        "            if len(transformed['bboxes']) == 0:\n",
        "                # fallback\n",
        "                bbox_transformed = bbox_abs\n",
        "            else:\n",
        "                bbox_transformed = transformed['bboxes'][0]\n",
        "\n",
        "            # Albumentations resizes to 416×416, so normalize relative to 416\n",
        "            bbox_final = [\n",
        "                bbox_transformed[0] / 416,\n",
        "                bbox_transformed[1] / 416,\n",
        "                bbox_transformed[2] / 416,\n",
        "                bbox_transformed[3] / 416,\n",
        "            ]\n",
        "\n",
        "            bbox = torch.tensor(bbox_final, dtype=torch.float32)\n",
        "\n",
        "        else:\n",
        "            # if no transform: normalize relative to original\n",
        "            bbox_norm = [\n",
        "                bbox_abs[0] / img_w,\n",
        "                bbox_abs[1] / img_h,\n",
        "                bbox_abs[2] / img_w,\n",
        "                bbox_abs[3] / img_h,\n",
        "            ]\n",
        "            image = transforms.ToTensor()(image)\n",
        "            bbox = torch.tensor(bbox_norm, dtype=torch.float32)\n",
        "\n",
        "        return image, bbox, license_plate_string, torch.tensor(char_indices, dtype=torch.long)\n",
        "\n",
        "    def _extract_annotations_from_filename(self, filename):\n",
        "        # Remove the file extension\n",
        "        filename_no_ext = os.path.splitext(filename)[0]\n",
        "        # print(\"\\nfilename_no_ext\", filename_no_ext)\n",
        "\n",
        "        # Split the filename into the seven fields\n",
        "        parts = filename_no_ext.split('-')\n",
        "\n",
        "       # Extract Bounding box coordinates (field 3, index 2)\n",
        "        # The format is \"x1&y1_x2&y2\" (left-up and right-bottom)\n",
        "        bbox_coords_str = parts[2].split('_')\n",
        "        # print(\"bbox_coords_str\", bbox_coords_str) # Added for debugging\n",
        "\n",
        "        # Split by '&' to get x and y for left-up and right-bottom\n",
        "        left_up = [int(coord) for coord in bbox_coords_str[0].split(FILENAME_SPLITTER)] # should split on &\n",
        "        right_bottom = [int(coord) for coord in bbox_coords_str[1].split(FILENAME_SPLITTER)] # should split on &\n",
        "\n",
        "        # Bounding box in (x1, y1, x2, y2) format\n",
        "        bbox = (left_up[0], left_up[1], right_bottom[0], right_bottom[1])\n",
        "        # print(\"Bounding box: (\", left_up[0], \",\", left_up[1], \",\", right_bottom[0], \",\", right_bottom[1], \")\") # Added for debugging\n",
        "\n",
        "        # Extract Four vertices locations (field 4, index 3)\n",
        "        # The format is \"x1&y1_x2&y2_x3&y3_x4&y4\" (starting from right-bottom)\n",
        "        vertices_str = parts[3].split('_')\n",
        "        vertices = []\n",
        "        for vertex_str in vertices_str:\n",
        "            vertices.append([int(coord) for coord in vertex_str.split(FILENAME_SPLITTER)]) # should split on &\n",
        "        # vertices will be a list of [x, y] pairs\n",
        "\n",
        "        # Extract License plate number (field 5, index 4)\n",
        "        # The format is \"0_0_22_27_27_33_16\" (indices of characters)\n",
        "        char_indices_str = parts[4].split('_') # Corrected index to 4\n",
        "        char_indices = [int(index) for index in char_indices_str]\n",
        "        # print(\"Char indices:\", char_indices) # Added for debugging\n",
        "\n",
        "        # Extract Brightness (field 6, index 5)\n",
        "        brightness = int(parts[5]) # Corrected index to 5\n",
        "\n",
        "        # Extract Blurriness (field 7, index 6)\n",
        "        blurriness = int(parts[6]) # Corrected index to 6\n",
        "\n",
        "        # You can also extract Area (field 1, index 0) and Tilt degree (field 2, index 1) if needed.\n",
        "        # area = float(parts[0])\n",
        "        # tilt_degree = [int(deg) for deg in parts[1].split('_')]\n",
        "\n",
        "\n",
        "        # Return the extracted information.\n",
        "        # For a baseline model, you'll likely need the bounding box for detection\n",
        "        # and the character indices for recognition.\n",
        "        return bbox, char_indices\n",
        "\n",
        "    def _indices_to_string(self, char_indices):\n",
        "        # Map indices to characters based on their position\n",
        "        # The format is province, alphabet, and then five from ads\n",
        "        license_plate = \"\"\n",
        "        if len(char_indices) > 0:\n",
        "            license_plate += self.provinces[char_indices[0]]\n",
        "        if len(char_indices) > 1:\n",
        "            license_plate += self.alphabets[char_indices[1]]\n",
        "        for i in range(2, min(len(char_indices), 7)): # Assuming 7 characters total\n",
        "            license_plate += self.ads[char_indices[i]]\n",
        "\n",
        "        return license_plate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:34:03.662165Z",
          "iopub.status.busy": "2025-07-03T09:34:03.661745Z",
          "iopub.status.idle": "2025-07-03T09:34:03.665887Z",
          "shell.execute_reply": "2025-07-03T09:34:03.665301Z",
          "shell.execute_reply.started": "2025-07-03T09:34:03.662145Z"
        },
        "id": "5On4l3_A2Mt5"
      },
      "outputs": [],
      "source": [
        "class SmallSubsetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, max_samples=100, random_subset=False):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.max_samples = min(max_samples, len(base_dataset))\n",
        "\n",
        "        if random_subset:\n",
        "            self.image_files = random.sample(base_dataset.image_files, self.max_samples)\n",
        "        else:\n",
        "            self.image_files = base_dataset.image_files[:self.max_samples]\n",
        "\n",
        "        self.provinces = base_dataset.provinces\n",
        "        self.alphabets = base_dataset.alphabets\n",
        "        self.ads = base_dataset.ads\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # locate the original dataset index\n",
        "        img_file = self.image_files[idx]\n",
        "        original_idx = self.base_dataset.image_files.index(img_file)\n",
        "        return self.base_dataset[original_idx]\n",
        "    def _indices_to_string(self, indices):\n",
        "        return self.base_dataset._indices_to_string(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:33:32.357323Z",
          "iopub.status.busy": "2025-07-03T09:33:32.357069Z",
          "iopub.status.idle": "2025-07-03T09:33:32.361264Z",
          "shell.execute_reply": "2025-07-03T09:33:32.360705Z",
          "shell.execute_reply.started": "2025-07-03T09:33:32.357309Z"
        },
        "id": "13JI8wU_PvmC"
      },
      "outputs": [],
      "source": [
        "# Define your transforms\n",
        "height = 416\n",
        "width = 416\n",
        "\n",
        "\"\"\"\n",
        "transform = transforms.Compose([\n",
        "transforms.Resize((height, width)), # Resize to a fixed size (choose appropriate dimensions)\n",
        "transforms.ToTensor(), # Convert PIL Image to PyTorch tensor\n",
        "transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats - adjust if needed\n",
        "# Add data augmentation transforms as well (e.g., RandomRotation, ColorJitter)\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "transforms.Resize((height, width)), # Resize to a fixed size (choose appropriate dimensions)\n",
        "transforms.ToTensor(), # Convert PIL Image to PyTorch tensor\n",
        "])\n",
        "\"\"\"\n",
        "\n",
        "transform_train = A.Compose([\n",
        "    A.Resize(height=416, width=416),  # fixed image size\n",
        "    A.OneOf([\n",
        "        A.MotionBlur(p=0.2),\n",
        "        A.MedianBlur(blur_limit=3, p=0.1),\n",
        "        A.GaussianBlur(p=0.1),\n",
        "        A.GaussNoise(p=0.2)\n",
        "    ], p=0.3),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.02, scale_limit=0.1, rotate_limit=5,\n",
        "        border_mode=0, p=0.5\n",
        "    ),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format='pascal_voc',\n",
        "        label_fields=['class_labels'],\n",
        "        min_visibility=0.0,            # keeps bboxes with at least 30% visible area\n",
        "        #filter_lost_elements=True      # filters out bboxes lost after transform\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "transform_val = A.Compose([\n",
        "    A.Resize(height=416, width=416),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-03T10:39:10.802431Z",
          "iopub.status.busy": "2025-07-03T10:39:10.802080Z",
          "iopub.status.idle": "2025-07-03T10:39:12.769491Z",
          "shell.execute_reply": "2025-07-03T10:39:12.768914Z",
          "shell.execute_reply.started": "2025-07-03T10:39:10.802417Z"
        },
        "id": "wrT4K132UtJ-",
        "outputId": "255c0c23-3ff9-4cc2-80d5-6095369ba0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100000 image files in the data directory.\n",
            "No split file provided, using all image files.\n",
            "Found 99996 image files in the data directory.\n",
            "No split file provided, using all image files.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CCPDDataset(data_dir=TRAIN_DATASET, transform=transform_train)\n",
        "val_dataset = CCPDDataset(data_dir=VAL_DATASET, transform=transform_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cofwyWuGGQWE"
      },
      "source": [
        "If you want to take a subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T10:39:14.379218Z",
          "iopub.status.busy": "2025-07-03T10:39:14.378667Z",
          "iopub.status.idle": "2025-07-03T10:39:14.383439Z",
          "shell.execute_reply": "2025-07-03T10:39:14.382905Z",
          "shell.execute_reply.started": "2025-07-03T10:39:14.379198Z"
        },
        "id": "Y6aGkJPL2TkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88144df4-d276-4953-865c-e4d58b08687d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset of train dataset has size 1000\n",
            "Subset of evaluation dataset has size 1000\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SmallSubsetDataset(train_dataset, max_samples=NUM_SAMPLES)\n",
        "val_dataset = SmallSubsetDataset(val_dataset, max_samples=NUM_SAMPLES, random_subset=True)\n",
        "print(f\"Subset of train dataset has size {len(train_dataset)}\")\n",
        "print(f\"Subset of evaluation dataset has size {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T09:35:07.405260Z",
          "iopub.status.busy": "2025-07-03T09:35:07.404904Z",
          "iopub.status.idle": "2025-07-03T09:35:07.408643Z",
          "shell.execute_reply": "2025-07-03T09:35:07.408072Z",
          "shell.execute_reply.started": "2025-07-03T09:35:07.405244Z"
        },
        "id": "uN710ul-nhvY"
      },
      "outputs": [],
      "source": [
        "def custom_collate(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    bboxes = torch.stack([item[1] for item in batch])\n",
        "    license_plate_strings = [item[2] for item in batch]  # list of strings, keep as list\n",
        "    char_indices = torch.stack([item[3] for item in batch])\n",
        "\n",
        "    return images, bboxes, license_plate_strings, char_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-03T09:35:09.549732Z",
          "iopub.status.busy": "2025-07-03T09:35:09.549410Z",
          "iopub.status.idle": "2025-07-03T09:35:09.553810Z",
          "shell.execute_reply": "2025-07-03T09:35:09.553177Z",
          "shell.execute_reply.started": "2025-07-03T09:35:09.549714Z"
        },
        "id": "3HkGte_R1vgG",
        "outputId": "e9797322-df62-4b1c-ba5f-d70a4a7f055a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the training dataset: 1000\n",
            "Number of images in the evaluation dataset: 1000\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "print(f\"Number of images in the training dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of images in the evaluation dataset: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WGopU37nvNq",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T11:47:11.592507Z",
          "iopub.status.busy": "2025-07-02T11:47:11.592187Z",
          "iopub.status.idle": "2025-07-02T11:47:12.111536Z",
          "shell.execute_reply": "2025-07-02T11:47:12.111077Z",
          "shell.execute_reply.started": "2025-07-02T11:47:11.592493Z"
        },
        "id": "hH2bGBkegS4D",
        "outputId": "b482bba3-7ed2-4432-99d5-b2800d79ef06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "---*** TRAINING DATASET ***---\n",
            "\n",
            "--- Sample 0 ---\n",
            "Original filename: 0111494252874-90_82-268&522_482&587-498&599_269&588_247&509_476&520-0_0_11_32_32_30_17-155-33.jpg\n",
            "Bounding box: tensor([0.30314, 0.42485, 0.62072, 0.49702])\n",
            "License plate string: 皖AM886T\n",
            "Character indices tensor: tensor([ 0,  0, 11, 32, 32, 30, 17])\n",
            "\n",
            "--- Sample 1 ---\n",
            "Original filename: 019838362069-90_74-212&484_511&569-510&570_221&555_191&470_480&485-0_0_21_27_1_31_31-75-29.jpg\n",
            "Bounding box: tensor([0.29365, 0.40129, 0.68436, 0.50042])\n",
            "License plate string: 皖AX3B77\n",
            "Character indices tensor: tensor([ 0,  0, 21, 27,  1, 31, 31])\n",
            "\n",
            "--- Sample 2 ---\n",
            "Original filename: 0125778256705-89_86-230&561_421&629-421&625_226&625_223&561_418&561-0_0_1_1_31_29_32-143-79.jpg\n",
            "Bounding box: tensor([0.29853, 0.47563, 0.58324, 0.55979])\n",
            "License plate string: 皖ABB758\n",
            "Character indices tensor: tensor([ 0,  0,  1,  1, 31, 29, 32])\n",
            "\n",
            "--- Sample 3 ---\n",
            "Original filename: 0316594827586-103_78-258&506_507&645-500&645_250&579_256&498_506&564-0_0_7_33_3_32_33-136-58.jpg\n",
            "Bounding box: tensor([0.34818, 0.41126, 0.70475, 0.56215])\n",
            "License plate string: 皖AH9D89\n",
            "Character indices tensor: tensor([ 0,  0,  7, 33,  3, 32, 33])\n",
            "\n",
            "--- Sample 4 ---\n",
            "Original filename: 00801724137931-91_84-262&454_413&519-414&526_255&511_261&449_420&464-0_0_4_33_25_33_1-100-8.jpg\n",
            "Bounding box: tensor([0.42639, 0.39138, 0.63611, 0.44741])\n",
            "License plate string: 皖AE919B\n",
            "Character indices tensor: tensor([ 0,  0,  4, 33, 25, 33,  1])\n",
            "\n",
            "\n",
            "---*** EVALUATION DATASET ***---\n",
            "\n",
            "--- Sample 0 ---\n",
            "Original filename: 0111973180076-89_89-284&444_462&514-471&516_288&520_282&446_465&442-0_0_24_22_32_30_31-96-23.jpg\n",
            "Bounding box: tensor([0.39444, 0.38276, 0.64167, 0.44310])\n",
            "License plate string: 皖A0Y867\n",
            "Character indices tensor: tensor([ 0,  0, 24, 22, 32, 30, 31])\n",
            "\n",
            "--- Sample 1 ---\n",
            "Original filename: 0458010057471-94_81-193&557_523&707-532&716_207&666_182&546_507&596-10_3_24_26_24_4_10-94-55.jpg\n",
            "Bounding box: tensor([0.26806, 0.48017, 0.72639, 0.60948])\n",
            "License plate string: 苏D020EL\n",
            "Character indices tensor: tensor([10,  3, 24, 26, 24,  4, 10])\n",
            "\n",
            "--- Sample 2 ---\n",
            "Original filename: 0334913793103-97_81-209&332_476&463-486&464_216&414_205&325_475&375-0_0_23_32_10_27_30-149-54.jpg\n",
            "Bounding box: tensor([0.29028, 0.28621, 0.66111, 0.39914])\n",
            "License plate string: 皖AZ8L36\n",
            "Character indices tensor: tensor([ 0,  0, 23, 32, 10, 27, 30])\n",
            "\n",
            "--- Sample 3 ---\n",
            "Original filename: 0296000957855-91_91-181&538_467&648-453&644_191&645_200&524_462&523-0_0_23_32_30_25_13-130-50.jpg\n",
            "Bounding box: tensor([0.25139, 0.46379, 0.64861, 0.55862])\n",
            "License plate string: 皖AZ861P\n",
            "Character indices tensor: tensor([ 0,  0, 23, 32, 30, 25, 13])\n",
            "\n",
            "--- Sample 4 ---\n",
            "Original filename: 0132100095786-90_86-323&507_530&581-527&590_328&579_317&507_516&518-0_0_16_25_30_24_17-157-11.jpg\n",
            "Bounding box: tensor([0.44861, 0.43707, 0.73611, 0.50086])\n",
            "License plate string: 皖AS160T\n",
            "Character indices tensor: tensor([ 0,  0, 16, 25, 30, 24, 17])\n"
          ]
        }
      ],
      "source": [
        "# Access a few samples directly from the dataset\n",
        "num_samples_to_check = 5 # Number of samples to inspect\n",
        "\n",
        "# Function to display a tensor image\n",
        "def imshow(img):\n",
        "    # Unnormalize the image using the mean and std from the transform\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img.numpy().transpose((1, 2, 0)) + mean\n",
        "    img = np.clip(img, 0, 1) # Clip values to be between 0 and 1\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off') # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\\n---*** TRAINING DATASET ***---\")\n",
        "for i in range(num_samples_to_check):\n",
        "    image, bbox, license_plate_string, char_indices_tensor = train_dataset[i]\n",
        "\n",
        "    print(f\"\\n--- Sample {i} ---\")\n",
        "    print(\"Original filename:\", os.path.basename(train_dataset.image_files[i]))\n",
        "    print(\"Bounding box:\", bbox)\n",
        "    print(\"License plate string:\", license_plate_string)\n",
        "    print(\"Character indices tensor:\", char_indices_tensor)\n",
        "    imshow(image)\n",
        "\n",
        "print(\"\\n\\n---*** EVALUATION DATASET ***---\")\n",
        "for i in range(num_samples_to_check):\n",
        "    image, bbox, license_plate_string, char_indices_tensor = val_dataset[i]\n",
        "\n",
        "    print(f\"\\n--- Sample {i} ---\")\n",
        "    print(\"Original filename:\", os.path.basename(val_dataset.image_files[i]))\n",
        "    print(\"Bounding box:\", bbox)\n",
        "    print(\"License plate string:\", license_plate_string)\n",
        "    print(\"Character indices tensor:\", char_indices_tensor)\n",
        "    imshow(image)\n",
        "\n",
        "# Get a single batch from the dataloader\n",
        "# for images, bboxes, license_plates, char_indices_tensor in train_dataloader:\n",
        "#     print(\"\\n--- First Batch Labels ---\")\n",
        "#     print(\"Bounding box batch:\", bboxes)\n",
        "#     print(\"License plate batch:\", license_plates)\n",
        "#     print(\"Character indices batch tensor:\", char_indices_tensor)\n",
        "#     # Display the first image in the batch\n",
        "#     if len(images) > 0:\n",
        "#         print(\"Displaying the first image in the batch:\")\n",
        "#         imshow(images[0])\n",
        "\n",
        "#     # Break after the first batch\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U380SvxItSGw",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Baseline model (detection)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "# Install required packages\n",
        "subprocess.run([\"pip\", \"install\", \"-r\", f\"{BASE_DIR}/yolov5/requirements.txt\"])"
      ],
      "metadata": {
        "id": "RxfgXqMu5Z9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f2ed47-a3b2-452d-eefd-1e355e37fd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17511, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 17511 (delta 5), reused 0 (delta 0), pack-reused 17493 (from 3)\u001b[K\n",
            "Receiving objects: 100% (17511/17511), 16.65 MiB | 28.56 MiB/s, done.\n",
            "Resolving deltas: 100% (11996/11996), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', '-r', '/content/yolov5/requirements.txt'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)\n",
        "model = torch.hub.load(\"ultralytics/yolov5\", YOLO_MODEL)  # Default: yolov5s\n",
        "\n",
        "\"\"\"\n",
        "# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\n",
        "img = \"https://ultralytics.com/images/zidane.jpg\"  # Example image\n",
        "\n",
        "# Perform inference (handles batching, resizing, normalization automatically)\n",
        "results = model(img)\n",
        "\n",
        "# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\n",
        "results.print()  # Print results to console\n",
        "results.show()  # Display results in a window\n",
        "results.save()  # Save results to runs/detect/exp\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M8oNCMlh52lp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "42ca6ae1-ad26-4341-9957-909f484ab4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2025-7-13 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 176MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # Example image\\n\\n# Perform inference (handles batching, resizing, normalization automatically)\\nresults = model(img)\\n\\n# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\\nresults.print()  # Print results to console\\nresults.show()  # Display results in a window\\nresults.save()  # Save results to runs/detect/exp\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old baseline model"
      ],
      "metadata": {
        "id": "GpixvnLcpgaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiGOZvtjx_6I"
      },
      "outputs": [],
      "source": [
        "class LicensePlateDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=1, padding=1),   # 224x224 → 224x224\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                            # 112x112\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, stride=1, padding=1),  # 112x112\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                            # 56x56\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, stride=1, padding=1), # 56x56\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                            # 28x28\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride=1, padding=1),# 28x28\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)                     # 1x1\n",
        "        )\n",
        "        self.regressor = nn.Linear(256, 4)  # x1,y1,x2,y2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.regressor(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7laHBSKiuwr4",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Training Loop (detection)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_yolo(subset_dataset, output_dir):\n",
        "    image_dir = os.path.join(output_dir, 'images')\n",
        "    label_dir = os.path.join(output_dir, 'labels')\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "    for i in tqdm(range(len(subset_dataset)), desc=f\"Exporting to {output_dir}\"):\n",
        "        image, bbox, _, _ = subset_dataset[i]\n",
        "\n",
        "        # Get original path\n",
        "        if hasattr(subset_dataset, 'image_files'):\n",
        "            img_path = subset_dataset.image_files[i]\n",
        "        elif hasattr(subset_dataset, 'base_dataset') and hasattr(subset_dataset.base_dataset, 'image_files'):\n",
        "            img_path = subset_dataset.base_dataset.image_files[i]\n",
        "        else:\n",
        "            raise ValueError(\"Cannot locate image path\")\n",
        "\n",
        "        # Save image to new folder\n",
        "        new_img_path = os.path.join(image_dir, os.path.basename(img_path))\n",
        "        shutil.copyfile(img_path, new_img_path)\n",
        "\n",
        "        # Convert bbox to YOLO format (x_center, y_center, width, height)\n",
        "        img_pil = Image.open(img_path).convert('RGB')\n",
        "        img_w, img_h = img_pil.size\n",
        "\n",
        "        x1 = bbox[0].item() * img_w\n",
        "        y1 = bbox[1].item() * img_h\n",
        "        x2 = bbox[2].item() * img_w\n",
        "        y2 = bbox[3].item() * img_h\n",
        "\n",
        "        x_center = (x1 + x2) / 2 / img_w\n",
        "        y_center = (y1 + y2) / 2 / img_h\n",
        "        width = (x2 - x1) / img_w\n",
        "        height = (y2 - y1) / img_h\n",
        "\n",
        "        # Save label\n",
        "        label_filename = os.path.basename(img_path).replace('.jpg', '.txt')\n",
        "        label_path = os.path.join(label_dir, label_filename)\n",
        "\n",
        "        with open(label_path, 'w') as f:\n",
        "            f.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n"
      ],
      "metadata": {
        "id": "7bUo-jwdpd1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the whole dataset"
      ],
      "metadata": {
        "id": "Xq1HxZhW-IoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_to_yolo(train_dataset)\n",
        "export_to_yolo(val_dataset)"
      ],
      "metadata": {
        "id": "xjhgJmUB-Klf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data file\n",
        "ccpd_yaml_path = f\"{BASE_DIR}/ccpd.yaml\"\n",
        "\n",
        "with open(ccpd_yaml_path, 'w') as f:\n",
        "    f.write(f\"\"\"train: {BASE_DIR}/CCPD2019/ccpd_train\n",
        "val: {BASE_DIR}/CCPD2019/ccpd_val\n",
        "\n",
        "nc: 1\n",
        "names: ['license_plate']\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "2wKEDLGPp22q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OR use a subset"
      ],
      "metadata": {
        "id": "xAjih_K7-NGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export labels\n",
        "export_to_yolo(train_dataset, \"ccpd_subset/train\")\n",
        "export_to_yolo(val_dataset, \"ccpd_subset/val\")"
      ],
      "metadata": {
        "id": "b6QgiXpJp-Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05cf7f6-4a0b-46cc-e7a3-b79a9ac7e787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting to ccpd_subset/train: 100%|██████████| 1000/1000 [00:20<00:00, 49.77it/s]\n",
            "Exporting to ccpd_subset/val: 100%|██████████| 1000/1000 [00:15<00:00, 63.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ccpd_yaml_path = f\"{BASE_DIR}/ccpd_subset.yaml\"\n",
        "\n",
        "with open(ccpd_yaml_path, 'w') as f:\n",
        "    f.write(f\"\"\"train: {BASE_DIR}/ccpd_subset/train\n",
        "val: {BASE_DIR}/ccpd_subset/val\n",
        "\n",
        "nc: 1\n",
        "names: ['license_plate']\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "PWxbp49N-U4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "g8mIHuJFE6M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "subprocess.run([\n",
        "    \"python\",\n",
        "    \"-W\", \"ignore::FutureWarning\",\n",
        "    \"-W\", \"ignore::DeprecationWarning\",\n",
        "    f\"{BASE_DIR}/yolov5/train.py\",\n",
        "    \"--img\", \"416\",\n",
        "    \"--batch\", f\"{BATCH_SIZE}\",\n",
        "    \"--epochs\", f\"{NUM_EPOCHS}\",\n",
        "    \"--data\", f\"{ccpd_yaml_path}\",\n",
        "    \"--weights\", f\"{YOLO_MODEL}.pt\",\n",
        "    \"--name\", \"ccpd_yolo_baseline\"\n",
        "])"
      ],
      "metadata": {
        "id": "1yIJ--fSqJMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bdedaf-a47a-4437-fac4-b9139d2df6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['python', '-W', 'ignore::FutureWarning', '-W', 'ignore::DeprecationWarning', '/content/yolov5/train.py', '--img', '416', '--batch', '32', '--epochs', '30', '--data', '/content/ccpd_subset.yaml', '--weights', 'yolov5s.pt', '--name', 'ccpd_yolo_baseline'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop"
      ],
      "metadata": {
        "id": "PiLlQjB3pm0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T11:51:05.029315Z",
          "iopub.status.busy": "2025-07-02T11:51:05.028983Z",
          "iopub.status.idle": "2025-07-02T12:56:44.720556Z",
          "shell.execute_reply": "2025-07-02T12:56:44.717655Z",
          "shell.execute_reply.started": "2025-07-02T11:51:05.029300Z"
        },
        "id": "FI5sN4Fptp9a",
        "outputId": "5762776f-643e-4441-eb3b-4d3fb5fc7280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected x_min for bbox [      -46.5       197.5       498.5       778.5           0] to be in the range [0.0, 1.0], got -46.5.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-1333005669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m#print(\"images.shape:\", images.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#print(\"bboxes.shape:\", bboxes.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-3586007165.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moriginal_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-10-3586007165.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moriginal_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-9-1515224858.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             transformed = self.transform(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mbboxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox_abs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m_preprocess_processors\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_data_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m             \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label_fields_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     def check_and_convert(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mcheck_and_convert\u001b[0;34m(self, data, shape, direction)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# Finally check the remaining boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, data, shape)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \"\"\"\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mcheck_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_from_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mShapeType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/augmentations/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/albumentations/core/bbox_utils.py\u001b[0m in \u001b[0;36mcheck_bboxes\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0minvalid_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"x_min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y_min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x_max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y_max\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mvalid_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0minvalid_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvalid_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mvalid_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0;34mf\"Expected {invalid_coord} for bbox {invalid_bbox} to be in the range [0.0, 1.0], got {invalid_value}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected x_min for bbox [      -46.5       197.5       498.5       778.5           0] to be in the range [0.0, 1.0], got -46.5."
          ]
        }
      ],
      "source": [
        "class GIoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        # pred_boxes and target_boxes are [B,4] in normalized coords: x1,y1,x2,y2\n",
        "\n",
        "        x1_p, y1_p, x2_p, y2_p = pred_boxes[:,0], pred_boxes[:,1], pred_boxes[:,2], pred_boxes[:,3]\n",
        "        x1_t, y1_t, x2_t, y2_t = target_boxes[:,0], target_boxes[:,1], target_boxes[:,2], target_boxes[:,3]\n",
        "\n",
        "        x1_i = torch.max(x1_p, x1_t)\n",
        "        y1_i = torch.max(y1_p, y1_t)\n",
        "        x2_i = torch.min(x2_p, x2_t)\n",
        "        y2_i = torch.min(y2_p, y2_t)\n",
        "\n",
        "        inter_area = (x2_i - x1_i).clamp(0) * (y2_i - y1_i).clamp(0)\n",
        "        area_p = (x2_p - x1_p).clamp(0) * (y2_p - y1_p).clamp(0)\n",
        "        area_t = (x2_t - x1_t).clamp(0) * (y2_t - y1_t).clamp(0)\n",
        "\n",
        "        union_area = area_p + area_t - inter_area + 1e-7\n",
        "        iou = inter_area / union_area\n",
        "\n",
        "        # enclosing box\n",
        "        x1_c = torch.min(x1_p, x1_t)\n",
        "        y1_c = torch.min(y1_p, y1_t)\n",
        "        x2_c = torch.max(x2_p, x2_t)\n",
        "        y2_c = torch.max(y2_p, y2_t)\n",
        "        area_c = (x2_c - x1_c) * (y2_c - y1_c) + 1e-7\n",
        "\n",
        "        giou = iou - (area_c - union_area) / area_c\n",
        "        loss = 1 - giou\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model = LicensePlateDetector().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "#criterion = nn.MSELoss()\n",
        "#criterion = nn.SmoothL1Loss()\n",
        "criterion = GIoULoss()\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "    for i, (images, bboxes, _, _) in enumerate(progress_bar):\n",
        "        #print(\"images.shape:\", images.shape)\n",
        "        #print(\"bboxes.shape:\", bboxes.shape)\n",
        "        images = images.to(device)\n",
        "        bboxes = bboxes.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, bboxes)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "    progress_bar.close()\n",
        "\n",
        "    scheduler.step()\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for images, bboxes, _, _ in val_dataloader:\n",
        "            images = images.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "            outputs = model(images)\n",
        "            vloss = criterion(outputs, bboxes)\n",
        "            val_loss += vloss.item()\n",
        "        val_loss /= len(val_dataloader)\n",
        "    print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1dtKO735sJf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Evaluation (detection)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run([\n",
        "    \"python\",\n",
        "    f\"{BASE_DIR}/yolov5/val.py\",                # path to val.py\n",
        "    \"--data\", f\"{ccpd_yaml_path}\",          # path to your dataset config\n",
        "    \"--weights\", \"yolov5/runs/train/ccpd_yolo_baseline11/weights/best.pt\",  # trained model\n",
        "    \"--img\", \"416\",                             # image size\n",
        "    \"--task\", \"val\"                             # or \"test\" if evaluating on test set\n",
        "])\n"
      ],
      "metadata": {
        "id": "Q0rxU6_bgW-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old evaluation"
      ],
      "metadata": {
        "id": "vauogNPSgYGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T12:56:59.748210Z",
          "iopub.status.busy": "2025-07-02T12:56:59.747544Z",
          "iopub.status.idle": "2025-07-02T12:56:59.762372Z",
          "shell.execute_reply": "2025-07-02T12:56:59.761731Z",
          "shell.execute_reply.started": "2025-07-02T12:56:59.748169Z"
        },
        "id": "8JxT88HE5xus"
      },
      "outputs": [],
      "source": [
        "def compute_iou(box1, box2):\n",
        "    # box1, box2 are tensors or lists: [x1, y1, x2, y2] normalized coords (0-1)\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    if union_area == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return inter_area / union_area\n",
        "\n",
        "def evaluate_model(model, dataloader, device, iou_threshold=IOU_THRESHOLD): # threshold in the article is 0.7 (was 0.5 before)\n",
        "    model.eval()\n",
        "    total_iou = 0\n",
        "    total_samples = 0\n",
        "    correct_detections = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "        for images, bboxes, _, _ in progress_bar:\n",
        "            images = images.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            # outputs shape: [batch_size, 4], predicted bounding boxes\n",
        "\n",
        "            for pred_box, gt_box in zip(outputs, bboxes):\n",
        "                iou = compute_iou(pred_box.cpu().numpy(), gt_box.cpu().numpy())\n",
        "                total_iou += iou\n",
        "                total_samples += 1\n",
        "                if iou >= iou_threshold:\n",
        "                    correct_detections += 1\n",
        "            #print(\"predicted box:\", outputs[0].cpu().numpy())\n",
        "            #print(\"ground truth:\", bboxes[0].cpu().numpy())\n",
        "\n",
        "    avg_iou = total_iou / total_samples if total_samples > 0 else 0\n",
        "    accuracy = correct_detections / total_samples if total_samples > 0 else 0\n",
        "    progress_bar.set_postfix(avg_iou=f\"{avg_iou:.4f}\", accuracy=f\"{accuracy:.4f}\")\n",
        "\n",
        "    print(f\"\\nAverage IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Detection accuracy (IoU >= {iou_threshold}): {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T12:57:02.396362Z",
          "iopub.status.busy": "2025-07-02T12:57:02.395903Z",
          "iopub.status.idle": "2025-07-02T12:59:12.084726Z",
          "shell.execute_reply": "2025-07-02T12:59:12.084254Z",
          "shell.execute_reply.started": "2025-07-02T12:57:02.396341Z"
        },
        "id": "tfvlqXYS99cU",
        "outputId": "fca1446c-e87c-40ec-b41c-1d01c3e67273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|█████████████████████████████████████████████████████████| 313/313 [02:09<00:00,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average IoU: 0.3699\n",
            "Detection accuracy (IoU >= 0.5): 0.3237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, val_dataloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OLCbAe3sOxz",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Visualize evaluation results (detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-02T13:01:46.701361Z",
          "iopub.status.busy": "2025-07-02T13:01:46.700922Z",
          "iopub.status.idle": "2025-07-02T13:01:46.708308Z",
          "shell.execute_reply": "2025-07-02T13:01:46.707600Z",
          "shell.execute_reply.started": "2025-07-02T13:01:46.701344Z"
        },
        "id": "jx4jYd_k7Xd2"
      },
      "outputs": [],
      "source": [
        "def visualize_batch_predictions(images, gt_bboxes, pred_bboxes, max_samples=4):\n",
        "    # images: batch tensor [B, 3, H, W]\n",
        "    # gt_bboxes, pred_bboxes: batch tensors [B, 4] with normalized coords\n",
        "    # max_samples: number of images to display\n",
        "\n",
        "    batch_size = min(len(images), max_samples)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        visualize_prediction(images[i], gt_bboxes[i], pred_bboxes[i])\n",
        "\n",
        "def visualize_random_predictions(images, gt_bboxes, pred_bboxes, max_samples=4):\n",
        "    batch_size = len(images)\n",
        "    indices = random.sample(range(batch_size), min(max_samples, batch_size))\n",
        "\n",
        "    for i in indices:\n",
        "        visualize_prediction(images[i], gt_bboxes[i], pred_bboxes[i])\n",
        "\n",
        "def visualize_prediction(image_tensor, gt_bbox, pred_bbox):\n",
        "    # image_tensor: tensor [3, H, W], normalized\n",
        "    # gt_bbox, pred_bbox: normalized [x1, y1, x2, y2]\n",
        "\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "\n",
        "    image = image_tensor * std + mean\n",
        "    image = image.clamp(0, 1).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(image)\n",
        "\n",
        "    h, w, _ = image.shape\n",
        "\n",
        "    # Convert normalized bboxes to absolute pixel coords\n",
        "    gt_x1, gt_y1, gt_x2, gt_y2 = gt_bbox\n",
        "    pred_x1, pred_y1, pred_x2, pred_y2 = pred_bbox\n",
        "\n",
        "    gt_rect = patches.Rectangle((gt_x1*w, gt_y1*h), (gt_x2 - gt_x1)*w, (gt_y2 - gt_y1)*h,\n",
        "                               linewidth=2, edgecolor='g', facecolor='none', label='Ground Truth')\n",
        "    pred_rect = patches.Rectangle((pred_x1*w, pred_y1*h), (pred_x2 - pred_x1)*w, (pred_y2 - pred_y1)*h,\n",
        "                                 linewidth=2, edgecolor='r', facecolor='none', label='Prediction')\n",
        "\n",
        "    ax.add_patch(gt_rect)\n",
        "    ax.add_patch(pred_rect)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "execution": {
          "iopub.execute_input": "2025-07-02T13:01:48.625730Z",
          "iopub.status.busy": "2025-07-02T13:01:48.625153Z",
          "iopub.status.idle": "2025-07-02T13:01:50.359975Z",
          "shell.execute_reply": "2025-07-02T13:01:50.358130Z",
          "shell.execute_reply.started": "2025-07-02T13:01:48.625714Z"
        },
        "id": "ZY-FOhWX-6Gg",
        "outputId": "c0fbc09c-b35e-4f80-a8a3-17a74b6f840d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2480976827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ],
      "source": [
        "images, gt_bboxes, _, _ = next(iter(val_dataloader))\n",
        "images = images.to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_bboxes = model(images).cpu()\n",
        "\n",
        "visualize_random_predictions(images.cpu(), gt_bboxes.cpu(), pred_bboxes, max_samples=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oxzGZCgF_WDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform detection"
      ],
      "metadata": {
        "id": "VQkeyFCM_chb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOLO_WEIGHTS_PATH = f'{BASE_DIR}/yolov5/runs/train/ccpd_yolo_baseline2/weights/best.pt'\n",
        "# Load the trained YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=YOLO_WEIGHTS_PATH)\n",
        "model.conf = 0.25  # confidence threshold (you can adjust)\n",
        "\n",
        "def detect_plates(image_path):\n",
        "    results = model(image_path)\n",
        "    detections = results.xyxy[0]  # [x1, y1, x2, y2, conf, class]\n",
        "\n",
        "    # Load image as numpy\n",
        "    image = cv2.imread(image_path)\n",
        "    cropped_images = []\n",
        "\n",
        "    for *box, conf, cls in detections:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cropped = image[y1:y2, x1:x2]\n",
        "        cropped_images.append((cropped, (x1, y1, x2, y2)))\n",
        "\n",
        "    return cropped_images  # list of (cropped_image, bbox)\n"
      ],
      "metadata": {
        "id": "6eNiw-0L_aQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17204219-a50b-4a61-fa2c-ac91de0425f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2025-7-13 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k22MEntnXtV",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Baseline model (recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szhe0P8p51Fz"
      },
      "source": [
        "Step 1: Data Preparation for the Recognition Model We need to feed the recognition model only the license plate part of the image. We'll use the ground truth bounding boxes to crop these out. First, a helper function to perform the cropping. The bounding boxes from CCPDDataset are normalized [x1, y1, x2, y2]. We'll need to denormalize them to pixel coordinates relative to the 416x416 input image and then crop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58dePddi54-Q"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def crop_license_plate(image_tensor_batch, gt_bboxes_batch, target_size=(64, 128)):\n",
        "    \"\"\"\n",
        "    Crops license plates from a batch of image tensors using ground truth bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image_tensor_batch (torch.Tensor): Batch of images, shape (B, C, H, W).\n",
        "                                           Assumed to be normalized if model expects normalized input.\n",
        "        gt_bboxes_batch (torch.Tensor): Batch of normalized bounding boxes (x1, y1, x2, y2),\n",
        "                                        shape (B, 4). Coordinates are relative to image_tensor_batch.\n",
        "        target_size (tuple): Desired (height, width) for the cropped plate.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Batch of cropped and resized license plate images.\n",
        "    \"\"\"\n",
        "    cropped_plates = []\n",
        "    _, _, H, W = image_tensor_batch.shape\n",
        "\n",
        "    for i in range(image_tensor_batch.size(0)):\n",
        "        img = image_tensor_batch[i]  # Single image (C, H, W)\n",
        "        bbox = gt_bboxes_batch[i]    # Single bbox (4)\n",
        "\n",
        "        # Denormalize bbox coordinates\n",
        "        x1 = int(bbox[0] * W)\n",
        "        y1 = int(bbox[1] * H)\n",
        "        x2 = int(bbox[2] * W)\n",
        "        y2 = int(bbox[3] * H)\n",
        "\n",
        "        # Ensure coordinates are within image bounds and valid\n",
        "        x1 = max(0, x1)\n",
        "        y1 = max(0, y1)\n",
        "        x2 = min(W, x2)\n",
        "        y2 = min(H, y2)\n",
        "\n",
        "        crop_width = x2 - x1\n",
        "        crop_height = y2 - y1\n",
        "\n",
        "        if crop_width <= 0 or crop_height <= 0:\n",
        "            # Handle invalid bbox, e.g., create a black image or skip\n",
        "            # For simplicity, create a black image of target_size\n",
        "            print(f\"Warning: Invalid bbox {bbox} resulted in zero/negative crop size. Using black image.\")\n",
        "            plate_crop = torch.zeros((img.size(0), target_size[0], target_size[1]), device=img.device)\n",
        "        else:\n",
        "            # Crop using torchvision.transforms.functional.crop\n",
        "            # crop expects (top, left, height, width)\n",
        "            plate_crop = TF.crop(img, top=y1, left=x1, height=crop_height, width=crop_width)\n",
        "\n",
        "        # Resize the cropped plate to the target size\n",
        "        plate_crop_resized = TF.resize(plate_crop, target_size, antialias=True) # antialias for newer torchvision\n",
        "        cropped_plates.append(plate_crop_resized)\n",
        "\n",
        "    return torch.stack(cropped_plates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPVfQayj59yy"
      },
      "source": [
        "Step 2: Design the Recognition Model (LicensePlateRecognizer) This model will take the cropped license plate image (e.g., 64x128 pixels) and output predictions for each of the 7 character positions. Provinces: Chinese characters representing provinces. Alphabets: A single letter. Ads: Alphanumeric characters (digits and letters, excluding 'I' and 'O' sometimes, but CCPD includes 'O'). The CCPDDataset already defines these lists: self.provinces (34 classes, including 'O' for other/unknown) self.alphabets (25 classes, including 'O') self.ads (35 classes: A-Z excluding I, plus 0-9, plus 'O') Our model will have a shared CNN backbone to extract features from the plate, and then separate fully connected \"heads\" to predict each character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlxIgohR5-m3"
      },
      "outputs": [],
      "source": [
        "class LicensePlateRecognizer(nn.Module):\n",
        "    def __init__(self, num_provinces, num_alphabets, num_ads, input_height=64, input_width=128):\n",
        "        super().__init__()\n",
        "        self.num_provinces = num_provinces\n",
        "        self.num_alphabets = num_alphabets\n",
        "        self.num_ads = num_ads\n",
        "\n",
        "        # CNN Backbone\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), # Keep size: Bx32x64x128\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),      # Bx32x32x64\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Bx64x32x64\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),      # Bx64x16x32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# Bx128x16x32\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2)), # Bx128x8x16\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# Bx256x8x16\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2,1))  # Bx256x4x16, trying to keep more width\n",
        "        )\n",
        "\n",
        "        # Calculate the flattened size after conv layers\n",
        "        # For input 64x128:\n",
        "        # After MaxPool1: 32x64\n",
        "        # After MaxPool2: 16x32\n",
        "        # After MaxPool3: 8x16\n",
        "        # After MaxPool4: 4x16\n",
        "        # So, flattened_size = 256 * 4 * 16\n",
        "        self.flattened_size = 256 * (input_height // 16) * (input_width // 8) # Generic calculation\n",
        "\n",
        "        # Fully Connected layers for each character\n",
        "        # These are \"heads\" for each position\n",
        "        self.fc_province = nn.Linear(self.flattened_size, num_provinces)\n",
        "        self.fc_alphabet = nn.Linear(self.flattened_size, num_alphabets)\n",
        "\n",
        "        # We need 5 'ads' characters\n",
        "        self.fc_ad1 = nn.Linear(self.flattened_size, num_ads)\n",
        "        self.fc_ad2 = nn.Linear(self.flattened_size, num_ads)\n",
        "        self.fc_ad3 = nn.Linear(self.flattened_size, num_ads)\n",
        "        self.fc_ad4 = nn.Linear(self.flattened_size, num_ads)\n",
        "        self.fc_ad5 = nn.Linear(self.flattened_size, num_ads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        # Get predictions for each character\n",
        "        out_province = self.fc_province(x)\n",
        "        out_alphabet = self.fc_alphabet(x)\n",
        "        out_ad1 = self.fc_ad1(x)\n",
        "        out_ad2 = self.fc_ad2(x)\n",
        "        out_ad3 = self.fc_ad3(x)\n",
        "        out_ad4 = self.fc_ad4(x)\n",
        "        out_ad5 = self.fc_ad5(x)\n",
        "\n",
        "        # Return as a list of tensors or stack them\n",
        "        # Stacking makes it easier to handle later: (Batch, NumChars, NumClassesPerCharType - not quite)\n",
        "        # Better to return a list, as class numbers differ\n",
        "        return [out_province, out_alphabet, out_ad1, out_ad2, out_ad3, out_ad4, out_ad5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjshDtonnrF",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Training (recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqu1b9NV6CZi"
      },
      "source": [
        "Step 3: Define the Loss Function We'll use nn.CrossEntropyLoss for each of the 7 character predictions and sum them up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGnFcbsA6ER4"
      },
      "outputs": [],
      "source": [
        "recognition_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def calculate_recognition_loss(predictions_list, char_indices_batch):\n",
        "    \"\"\"\n",
        "    Calculates the total cross-entropy loss for character recognition.\n",
        "\n",
        "    Args:\n",
        "        predictions_list (list of torch.Tensor): List of 7 tensors, where each tensor\n",
        "                                                 has shape (Batch_size, Num_classes_for_that_char_pos).\n",
        "                                                 These are raw logits from the model.\n",
        "        char_indices_batch (torch.Tensor): Ground truth character indices, shape (Batch_size, 7).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The total summed loss.\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    # char_indices_batch is (Batch, 7)\n",
        "    # predictions_list contains 7 tensors, e.g., predictions_list[0] is (Batch, num_provinces)\n",
        "\n",
        "    for i in range(7): # For each character position\n",
        "        # predictions_list[i] are the logits for the i-th character for all items in batch\n",
        "        # char_indices_batch[:, i] are the ground truth indices for the i-th char for all items in batch\n",
        "        loss = recognition_criterion(predictions_list[i], char_indices_batch[:, i])\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cWh6db6D5r"
      },
      "source": [
        "Step 4: Training Loop for the Recognition Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RECOGNITION_MODEL_PATH = f\"{BASE_DIR}/license_plate_recognizer.pth\"\n",
        "RECOGNITION_MODEL_PATH = f\"{BASE_DIR}/recognition_model.pth\""
      ],
      "metadata": {
        "id": "ybSRLwS6Fhkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rcy_0vR6UbD",
        "outputId": "bc843f55-9390-4d6a-e373-7e2db700fc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting recognition model training on cuda...\n",
            "Num provinces: 34, Num alphabets: 25, Num ads: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.10it/s, rec_loss=18.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Recognition Training: Avg. Loss: 18.5122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, rec_loss=16.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Recognition Training: Avg. Loss: 16.2975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.06it/s, rec_loss=16.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Recognition Training: Avg. Loss: 16.2088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.13it/s, rec_loss=16.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Recognition Training: Avg. Loss: 16.1489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.09it/s, rec_loss=16.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Recognition Training: Avg. Loss: 16.0797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.12it/s, rec_loss=16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Recognition Training: Avg. Loss: 16.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.01it/s, rec_loss=16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Recognition Training: Avg. Loss: 15.9513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, rec_loss=15.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Recognition Training: Avg. Loss: 15.9039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.11it/s, rec_loss=15.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Recognition Training: Avg. Loss: 15.8607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 (Recognition): 100%|██████████| 32/32 [00:15<00:00,  2.08it/s, rec_loss=15.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Recognition Training: Avg. Loss: 15.7740\n",
            "Saving recognition model to /content/recognition_model.pth\n",
            "Recognition model saved.\n",
            "Recognition model training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Get class counts from the dataset instance (make sure train_dataset is initialized)\n",
        "# If train_dataset is not yet fully initialized or accessible here, you might need to hardcode or pass these\n",
        "# For example, use the actual train_dataset instance\n",
        "num_provinces = len(train_dataset.provinces)\n",
        "num_alphabets = len(train_dataset.alphabets)\n",
        "num_ads = len(train_dataset.ads)\n",
        "\n",
        "recognition_model = LicensePlateRecognizer(\n",
        "    num_provinces=num_provinces,\n",
        "    num_alphabets=num_alphabets,\n",
        "    num_ads=num_ads,\n",
        "    input_height=64, # Target size for cropped plates\n",
        "    input_width=128  # Target size for cropped plates\n",
        ").to(device)\n",
        "\n",
        "recognition_optimizer = optim.Adam(recognition_model.parameters(), lr=1e-4) # Start with a smaller LR\n",
        "\n",
        "# --- Training Configuration ---\n",
        "RECOGNITION_EPOCHS = 10 # Adjust as needed\n",
        "PLATE_CROP_TARGET_SIZE = (64, 128) # (height, width) for cropped plates\n",
        "\n",
        "print(f\"Starting recognition model training on {device}...\")\n",
        "print(f\"Num provinces: {num_provinces}, Num alphabets: {num_alphabets}, Num ads: {num_ads}\")\n",
        "\n",
        "\n",
        "for epoch in range(RECOGNITION_EPOCHS):\n",
        "    recognition_model.train()\n",
        "    epoch_rec_loss = 0\n",
        "\n",
        "    # Use the same train_dataloader as for detection, but we'll process data differently\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{RECOGNITION_EPOCHS} (Recognition)\")\n",
        "\n",
        "    for i, (images_batch, gt_bboxes_batch, _, char_indices_batch) in enumerate(progress_bar):\n",
        "        images_batch = images_batch.to(device)       # Full images (B, C, 416, 416)\n",
        "        gt_bboxes_batch = gt_bboxes_batch.to(device) # Normalized GT bboxes (B, 4)\n",
        "        char_indices_batch = char_indices_batch.to(device) # GT char indices (B, 7)\n",
        "\n",
        "        # 1. Crop license plates using GT bounding boxes\n",
        "        # The 'images_batch' from dataloader are already transformed (resized, ToTensor, Normalized)\n",
        "        cropped_plates_batch = crop_license_plate(images_batch, gt_bboxes_batch, target_size=PLATE_CROP_TARGET_SIZE)\n",
        "        # cropped_plates_batch should be (B, C, PLATE_CROP_TARGET_SIZE[0], PLATE_CROP_TARGET_SIZE[1])\n",
        "\n",
        "        # 2. Forward pass through recognition model\n",
        "        recognition_optimizer.zero_grad()\n",
        "        # Ensure cropped_plates_batch are on the correct device (crop_license_plate should handle it if img.device is used)\n",
        "        predictions_list = recognition_model(cropped_plates_batch.to(device))\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        rec_loss = calculate_recognition_loss(predictions_list, char_indices_batch)\n",
        "\n",
        "        # 4. Backward pass and optimize\n",
        "        rec_loss.backward()\n",
        "        recognition_optimizer.step()\n",
        "\n",
        "        epoch_rec_loss += rec_loss.item()\n",
        "        progress_bar.set_postfix(rec_loss=epoch_rec_loss / (i + 1))\n",
        "\n",
        "    avg_epoch_rec_loss = epoch_rec_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Recognition Training: Avg. Loss: {avg_epoch_rec_loss:.4f}\")\n",
        "\n",
        "# Save the trained recognition model\n",
        "print(f\"Saving recognition model to {RECOGNITION_MODEL_PATH}\")\n",
        "torch.save(recognition_model.state_dict(), RECOGNITION_MODEL_PATH)\n",
        "print(\"Recognition model saved.\")\n",
        "\n",
        "print(\"Recognition model training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mDoqwj2nxIH",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Evaluation (recognition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jrCoRU46cSG"
      },
      "source": [
        "Step 5: Evaluation for the Recognition Model We need to see how well it's doing on the validation set. We'll calculate: Per-Character Accuracy: For each of the 7 positions, how often is the character correct? Full Plate Accuracy: How often is the entire 7-character plate string predicted correctly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZDevWkyn6pm"
      },
      "outputs": [],
      "source": [
        "# Create validation dataloader if not already done\n",
        "# Ensure val_dataset is created with the same transforms and uses the val split file\n",
        "# val_dataset = CCPDDataset(data_dir=VAL_DATASET, split_file=VAL_SPLIT_FILE, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rcWm171oKQ-"
      },
      "outputs": [],
      "source": [
        "#val_dataset_small_rec = SmallSubsetDataset(val_dataset, max_samples=NUM_SAMPLES) # Smaller subset for faster eval\n",
        "#val_dataloader_rec = DataLoader(val_dataset_small_rec, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omfnhhpH6d3m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def evaluate_recognition_model(model, dataloader, device, dataset_instance, crop_target_size):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "\n",
        "    # For per-character accuracy\n",
        "    correct_chars_counts = [0] * 7 # One counter for each of the 7 character positions\n",
        "    total_chars_counts = [0] * 7\n",
        "\n",
        "    # For full plate accuracy\n",
        "    correct_full_plates = 0\n",
        "\n",
        "    # To store some example predictions\n",
        "    example_predictions = []\n",
        "    max_examples_to_show = 5\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating Recognition Model\")\n",
        "        for images_batch, gt_bboxes_batch, gt_plate_strings_batch, char_indices_batch in progress_bar:\n",
        "            images_batch = images_batch.to(device)\n",
        "            gt_bboxes_batch = gt_bboxes_batch.to(device)\n",
        "            char_indices_batch = char_indices_batch.to(device) # (B, 7)\n",
        "\n",
        "            # 1. Crop plates\n",
        "            cropped_plates_batch = crop_license_plate(images_batch, gt_bboxes_batch, target_size=crop_target_size)\n",
        "\n",
        "            # 2. Get predictions\n",
        "            predictions_list = model(cropped_plates_batch.to(device)) # List of 7 tensors (B, Num_Classes)\n",
        "\n",
        "            batch_size = char_indices_batch.size(0)\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Process predictions for the batch\n",
        "            # predicted_indices_batch will be (B, 7)\n",
        "            predicted_indices_batch = torch.zeros_like(char_indices_batch)\n",
        "            for char_pos in range(7):\n",
        "                # predictions_list[char_pos] is (B, Num_Classes_for_this_pos)\n",
        "                # Take argmax to get the predicted class index\n",
        "                predicted_indices_batch[:, char_pos] = torch.argmax(predictions_list[char_pos], dim=1)\n",
        "\n",
        "            # 3. Compare with ground truth\n",
        "            for i in range(batch_size): # Iterate over samples in the batch\n",
        "                gt_indices_sample = char_indices_batch[i] # (7)\n",
        "                pred_indices_sample = predicted_indices_batch[i] # (7)\n",
        "\n",
        "                is_full_plate_correct = True\n",
        "                for char_pos in range(7):\n",
        "                    total_chars_counts[char_pos] += 1\n",
        "                    if gt_indices_sample[char_pos] == pred_indices_sample[char_pos]:\n",
        "                        correct_chars_counts[char_pos] += 1\n",
        "                    else:\n",
        "                        is_full_plate_correct = False\n",
        "\n",
        "                if is_full_plate_correct:\n",
        "                    correct_full_plates += 1\n",
        "\n",
        "                # Store some examples\n",
        "                if len(example_predictions) < max_examples_to_show:\n",
        "                    gt_str = dataset_instance._indices_to_string(gt_indices_sample.cpu().tolist())\n",
        "                    pred_str = dataset_instance._indices_to_string(pred_indices_sample.cpu().tolist())\n",
        "                    example_predictions.append({\"gt\": gt_str, \"pred\": pred_str, \"correct\": is_full_plate_correct})\n",
        "\n",
        "    # Calculate accuracies\n",
        "    per_char_accuracies = [(correct_chars_counts[j] / total_chars_counts[j] if total_chars_counts[j] > 0 else 0) for j in range(7)]\n",
        "    full_plate_accuracy = correct_full_plates / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    print(\"\\n--- Recognition Evaluation Results ---\")\n",
        "    for j in range(7):\n",
        "        print(f\"Character Position {j+1} Accuracy: {per_char_accuracies[j]:.4f}\")\n",
        "    print(f\"Full License Plate Accuracy: {full_plate_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Example Predictions ---\")\n",
        "    for ex in example_predictions:\n",
        "        print(f\"GT: {ex['gt']:<10} | Pred: {ex['pred']:<10} | Correct: {ex['correct']}\")\n",
        "\n",
        "    return per_char_accuracies, full_plate_accuracy\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWvRo5Mdn7TR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "print(\"\\nEvaluating recognition model on validation set...\")\n",
        "# Pass the original val_dataset instance for _indices_to_string method\n",
        "# and the crop_target_size used during training/inference\n",
        "evaluate_recognition_model(recognition_model, val_dataloader, device, val_dataset, PLATE_CROP_TARGET_SIZE)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTTa01F56gWU"
      },
      "source": [
        "Same evaluation but with images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxXvcQrA6huv"
      },
      "outputs": [],
      "source": [
        "def unnormalize_image(tensor_image, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "    \"\"\"Unnormalizes a tensor image.\"\"\"\n",
        "    # Create tensors for mean and std if they are not already\n",
        "    if not isinstance(mean, torch.Tensor):\n",
        "        mean = torch.tensor(mean, device=tensor_image.device).view(3, 1, 1)\n",
        "    if not isinstance(std, torch.Tensor):\n",
        "        std = torch.tensor(std, device=tensor_image.device).view(3, 1, 1)\n",
        "\n",
        "    unnormalized_image = tensor_image * std + mean\n",
        "    unnormalized_image = torch.clamp(unnormalized_image, 0, 1) # Clip to [0, 1] range\n",
        "    return unnormalized_image\n",
        "\n",
        "def evaluate_recognition_model(model, dataloader, device, dataset_instance, crop_target_size, num_examples_to_show=5):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "\n",
        "    # For per-character accuracy\n",
        "    correct_chars_counts = [0] * 7 # One counter for each of the 7 character positions\n",
        "    total_chars_counts = [0] * 7\n",
        "\n",
        "    # For full plate accuracy\n",
        "    correct_full_plates = 0\n",
        "\n",
        "    # To store some example predictions with images\n",
        "    example_predictions_with_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating Recognition Model\")\n",
        "        for images_batch, gt_bboxes_batch, gt_plate_strings_batch, char_indices_batch in progress_bar:\n",
        "            images_batch = images_batch.to(device)\n",
        "            gt_bboxes_batch = gt_bboxes_batch.to(device)\n",
        "            char_indices_batch = char_indices_batch.to(device) # (B, 7)\n",
        "\n",
        "            # 1. Crop plates\n",
        "            cropped_plates_batch = crop_license_plate(images_batch, gt_bboxes_batch, target_size=crop_target_size)\n",
        "            # cropped_plates_batch is (B, C, H_crop, W_crop) and should be on `device`\n",
        "\n",
        "            # 2. Get predictions\n",
        "            predictions_list = model(cropped_plates_batch) # List of 7 tensors (B, Num_Classes)\n",
        "\n",
        "            batch_size = char_indices_batch.size(0)\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Process predictions for the batch\n",
        "            predicted_indices_batch = torch.zeros_like(char_indices_batch)\n",
        "            for char_pos in range(7):\n",
        "                predicted_indices_batch[:, char_pos] = torch.argmax(predictions_list[char_pos], dim=1)\n",
        "\n",
        "            # 3. Compare with ground truth\n",
        "            for i in range(batch_size): # Iterate over samples in the batch\n",
        "                gt_indices_sample = char_indices_batch[i] # (7)\n",
        "                pred_indices_sample = predicted_indices_batch[i] # (7)\n",
        "\n",
        "                is_full_plate_correct = True\n",
        "                for char_pos in range(7):\n",
        "                    total_chars_counts[char_pos] += 1\n",
        "                    if gt_indices_sample[char_pos] == pred_indices_sample[char_pos]:\n",
        "                        correct_chars_counts[char_pos] += 1\n",
        "                    else:\n",
        "                        is_full_plate_correct = False\n",
        "\n",
        "                if is_full_plate_correct:\n",
        "                    correct_full_plates += 1\n",
        "\n",
        "                # Store some examples with their cropped images\n",
        "                if len(example_predictions_with_images) < num_examples_to_show:\n",
        "                    gt_str = dataset_instance._indices_to_string(gt_indices_sample.cpu().tolist())\n",
        "                    pred_str = dataset_instance._indices_to_string(pred_indices_sample.cpu().tolist())\n",
        "\n",
        "                    # Get the corresponding cropped plate image (move to CPU for plotting)\n",
        "                    # cropped_plates_batch[i] is (C, H_crop, W_crop)\n",
        "                    plate_img_tensor = cropped_plates_batch[i].cpu()\n",
        "\n",
        "                    example_predictions_with_images.append({\n",
        "                        \"image\": plate_img_tensor, # Store the tensor\n",
        "                        \"gt\": gt_str,\n",
        "                        \"pred\": pred_str,\n",
        "                        \"correct\": is_full_plate_correct\n",
        "                    })\n",
        "\n",
        "    # Calculate accuracies\n",
        "    per_char_accuracies = [(correct_chars_counts[j] / total_chars_counts[j] if total_chars_counts[j] > 0 else 0) for j in range(7)]\n",
        "    full_plate_accuracy = correct_full_plates / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    print(\"\\n--- Recognition Evaluation Results ---\")\n",
        "    for j in range(7):\n",
        "        print(f\"Character Position {j+1} Accuracy: {per_char_accuracies[j]:.4f}\")\n",
        "    print(f\"Full License Plate Accuracy: {full_plate_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Example Predictions with Images ---\")\n",
        "    if not example_predictions_with_images:\n",
        "        print(\"No examples to show.\")\n",
        "    else:\n",
        "        # Determine number of rows and columns for subplot\n",
        "        num_examples = len(example_predictions_with_images)\n",
        "        cols = min(num_examples, 3) # Max 3 columns\n",
        "        rows = (num_examples + cols - 1) // cols\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 3)) # Adjust figsize as needed\n",
        "        if num_examples == 1: # Handle case of single example for subplot indexing\n",
        "            axes = np.array([axes])\n",
        "        axes = axes.flatten() # Flatten to make indexing easier\n",
        "\n",
        "        for idx, ex in enumerate(example_predictions_with_images):\n",
        "            ax = axes[idx]\n",
        "            # Unnormalize and permute for display (C, H, W) -> (H, W, C)\n",
        "            img_to_show = unnormalize_image(ex['image']).permute(1, 2, 0).numpy()\n",
        "\n",
        "            ax.imshow(img_to_show)\n",
        "            title_color = 'green' if ex['correct'] else 'red'\n",
        "            ax.set_title(f\"GT: {ex['gt']}\\nPred: {ex['pred']}\", color=title_color, fontsize=10)\n",
        "            ax.axis('off')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_examples, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return per_char_accuracies, full_plate_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWKiez8N6jO9",
        "outputId": "1c7e9f57-3f7c-4448-8ebf-62dadaf01d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating recognition model on validation set with image examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Recognition Model: 100%|██████████| 32/32 [00:12<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Recognition Evaluation Results ---\n",
            "Character Position 1 Accuracy: 0.9620\n",
            "Character Position 2 Accuracy: 0.9180\n",
            "Character Position 3 Accuracy: 0.0340\n",
            "Character Position 4 Accuracy: 0.0850\n",
            "Character Position 5 Accuracy: 0.0930\n",
            "Character Position 6 Accuracy: 0.1220\n",
            "Character Position 7 Accuracy: 0.0980\n",
            "Full License Plate Accuracy: 0.0000\n",
            "\n",
            "--- Example Predictions with Images ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.962, 0.918, 0.034, 0.085, 0.093, 0.122, 0.098], 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Assuming all necessary variables are defined (recognition_model, val_dataloader_rec, device, val_dataset, PLATE_CROP_TARGET_SIZE)\n",
        "\n",
        "print(\"\\nEvaluating recognition model on validation set with image examples...\")\n",
        "evaluate_recognition_model(\n",
        "    recognition_model,\n",
        "    val_dataloader_rec,\n",
        "    device,\n",
        "    val_dataset, # Pass the CCPDDataset instance (e.g., val_dataset or val_dataset_small_rec.base_dataset if val_dataset_small_rec is a SmallSubsetDataset)\n",
        "    PLATE_CROP_TARGET_SIZE,\n",
        "    num_examples_to_show=10 # Or any number you prefer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_nuonURykPF"
      },
      "source": [
        "# YOLOv5 + PDLPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T11:12:22.856872Z",
          "iopub.status.busy": "2025-07-03T11:12:22.856450Z",
          "iopub.status.idle": "2025-07-03T11:12:22.995434Z",
          "shell.execute_reply": "2025-07-03T11:12:22.994929Z",
          "shell.execute_reply.started": "2025-07-03T11:12:22.856856Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5A7o7lpBEwG",
        "outputId": "7fab3f09-d14e-4dfa-8316-b07bae91ea40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 99996 image files in the data directory.\n",
            "No split file provided, using all image files.\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "#FOR TESTING\n",
        "NUM_SAMPLES = 10\n",
        "val_dataset = CCPDDataset(data_dir=VAL_DATASET, transform=transform_val)\n",
        "val_dataset = SmallSubsetDataset(val_dataset, max_samples=NUM_SAMPLES, random_subset=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-03T11:12:24.708837Z",
          "iopub.status.busy": "2025-07-03T11:12:24.708398Z",
          "iopub.status.idle": "2025-07-03T11:15:05.007771Z",
          "shell.execute_reply": "2025-07-03T11:15:05.006919Z",
          "shell.execute_reply.started": "2025-07-03T11:12:24.708820Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698,
          "referenced_widgets": [
            "0cd237cb105f49cbaab7ba135811d72a",
            "939356461876484294f0606676ade952",
            "c62d6a9d4989473282342e0e2e8af930",
            "4aef59bdcc894b8987bf237386f59342",
            "339ed757b6c743ef9f2e90cc65c8368e",
            "241aa8c95bf9463ebd566a26474898bc",
            "ec599cc444c745379ca8abcc284eeedd",
            "095aa5e7de034229bcd10e86bead36cf",
            "4f8ba623cbc3452ea380064d9372344c",
            "c14c6e71c6f541c3a1c97ec7236d1fe8",
            "653de4eca68c4f03ae228f1c28b9c2cc",
            "72759df25bb74d68bfb9faca58f3ec6d",
            "58227331dcd64728bf8c8f0ba007fcd4",
            "07e7f16d9b6c4b3bb1704d685cf43731",
            "e8d683eb100d46b682f2859b75117317",
            "bf545748d8164663a665dfed8229d23d",
            "a9ba52c30155481c8393c8f75be71cf7",
            "d6e8b5f9265045549bfc79086502fbe1",
            "0edae4b7118245ac9dc1414d6c261567",
            "d96563d0fd154d7b8dd4741daa246a12",
            "4a4f911ede8243ad989e277e2f55a5ba",
            "d468a0d7a79346ff80d32bda949a2768",
            "b75f089a807f4778a71527c4634c668b",
            "18b3618fdceb40ae9b51e6153f9d1ab7",
            "ae0af59a2ee24028b9ed851936276b92",
            "1329008df3ff41f7abbefc2430c67d1f",
            "e717af84c26c44ffaf20152eb98691a5",
            "62d5f9ac50244c95b84260a3928bd37f",
            "d3a42d2abd1b46128765caaa9b9fae89",
            "16c65936a5f441558c876bf8c2b5a9b0",
            "16f195f769a346278b5a888cfe3884fd",
            "95339dc5e0bc44eda8e91572ac975c68",
            "e2e7e1175ee74208b921f07270b1c1ae",
            "94fe8170c8654810aa64d3a8808e9a2b",
            "6af51f4af14241f09c9c3061a32504d6",
            "4b656117ba834a1292a70f5f65def991",
            "2f9a34b0696e40f28b239a8cb9b437c0",
            "05b2ee9401424103a01a9f51c9377c96",
            "dc52c77254874d89879b04132c4b6364",
            "8502fd3f4d4d4990b551e6e824691193",
            "c1457e1d155b4064a3455eef3cd8d27a",
            "9000d526bd06499cb4f333b20b584a55",
            "21c5d8fd13cd4f55924bf5f634c996a9",
            "88710e821aab4755971e630bdc311ba9",
            "89efe886a59e4b8580064fda9e2b8201",
            "d082f40bdc2f40f79df7686c720b7039",
            "8ad3244beca64eafb133401aae79ff78",
            "b23bd7ba6b36494497b180bd4fe64d4b",
            "c36e24c69bad4894bf2b5b92468bb16f",
            "223d9d015d1f41539cbac0313a46d4f3",
            "4df8af3ad1ba46619f33b307149aa2e9",
            "f5c3e24183ce44569c1dc9039e409111",
            "24b42c2b914b4549a6c7c904ec720f3d",
            "470ab6eb5dc845dfa9af39420fe8e473",
            "74b07e19996640cdbb1583c48df4cfc3"
          ]
        },
        "id": "ahlNjtXZBEwH",
        "outputId": "164489c3-5a9c-4d44-cae2-d287c86d49ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using font: ('Noto Sans SC', 'Thin')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
            "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cd237cb105f49cbaab7ba135811d72a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
            "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
            "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72759df25bb74d68bfb9faca58f3ec6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
            "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b75f089a807f4778a71527c4634c668b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
            "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94fe8170c8654810aa64d3a8808e9a2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
            "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89efe886a59e4b8580064fda9e2b8201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:  10%|█         | 1/10 [00:02<00:19,  2.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖A0N718'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  20%|██        | 2/10 [00:03<00:15,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AMR868'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  30%|███       | 3/10 [00:06<00:14,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖M33778'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  40%|████      | 4/10 [00:08<00:13,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AX6V78'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  50%|█████     | 5/10 [00:10<00:10,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AY3D16'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  60%|██████    | 6/10 [00:12<00:07,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AT1T28'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  70%|███████   | 7/10 [00:13<00:05,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AS7C86'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  80%|████████  | 8/10 [00:15<00:03,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AHH488'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  90%|█████████ | 9/10 [00:17<00:01,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AJ3S89'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 10/10 [00:19<00:00,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_text: '皖AJ660Z'\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Detection Precision: 1.000\n",
            "Detection Recall: 1.000\n",
            "Detection F1-score: 1.000\n",
            "OCR Accuracy (on detected plates): 0.800\n",
            "DONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "CONF_THRESHOLD = 0.25\n",
        "IOU_THRESHOLD = 0.5\n",
        "font_size = 24\n",
        "font_path = FONT_PATH\n",
        "save_dir = Path(\"results_with_ocr\")\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "try:\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "    print(f\"Using font: {font.getname()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load font: {e}\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "# Load models\n",
        "detector = yolov5.load('keremberke/yolov5m-license-plate')\n",
        "ocr_model = PaddleOCR(lang='ch', use_angle_cls=True)\n",
        "\n",
        "# Clear previous results\n",
        "for file in save_dir.glob(\"*\"):\n",
        "    if file.is_file():\n",
        "        file.unlink()\n",
        "\n",
        "# Metrics counters\n",
        "num_correct_detections = 0\n",
        "num_total_gt = 0\n",
        "num_total_pred = 0\n",
        "num_correct_ocr = 0\n",
        "\n",
        "def expand_box(x1, y1, x2, y2, img_w, img_h, ratio=0.1):\n",
        "    box_w = x2 - x1\n",
        "    box_h = y2 - y1\n",
        "    x1_new = max(0, int(x1 - ratio * box_w))\n",
        "    x2_new = min(img_w, int(x2 + ratio * box_w))\n",
        "    y1_new = max(0, int(y1 - ratio * box_h))\n",
        "    y2_new = min(img_h, int(y2 + ratio * box_h))\n",
        "    return x1_new, y1_new, x2_new, y2_new\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxAArea = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
        "    boxBArea = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
        "    if boxAArea + boxBArea - interArea == 0:\n",
        "        return 0\n",
        "    return interArea / (boxAArea + boxBArea - interArea)\n",
        "\n",
        "# Iterate over val dataloader\n",
        "total_images = len(val_dataset)  # total number of images in your dataset\n",
        "processed_images = 0\n",
        "\n",
        "with tqdm(total=total_images, desc=\"Processing\") as pbar:\n",
        "    for batch in val_dataloader:\n",
        "        images, gt_bboxes, gt_texts, _ = batch\n",
        "        batch_size = images.size(0)\n",
        "        for i in range(batch_size):\n",
        "            image_tensor = images[i]  # [C,H,W]\n",
        "            gt_bbox = gt_bboxes[i]\n",
        "            gt_text = gt_texts[i]\n",
        "\n",
        "            # Convert tensor to numpy BGR (denormalize if needed)\n",
        "            img_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # HWC RGB float32 [0,1]\n",
        "            img_np = (img_np * 255).astype(np.uint8)\n",
        "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "            h, w = img_bgr.shape[:2]\n",
        "\n",
        "            # Convert normalized gt bbox to absolute coords\n",
        "            x1_gt = int(gt_bbox[0].item() * w)\n",
        "            y1_gt = int(gt_bbox[1].item() * h)\n",
        "            x2_gt = int(gt_bbox[2].item() * w)\n",
        "            y2_gt = int(gt_bbox[3].item() * h)\n",
        "            gt_box = [x1_gt, y1_gt, x2_gt, y2_gt]\n",
        "            num_total_gt += 1\n",
        "\n",
        "            # Detection and rest of your code...\n",
        "            results = detector(img_bgr)\n",
        "            preds = results.pred[0].cpu().numpy()\n",
        "            preds = preds[preds[:,4] >= CONF_THRESHOLD]\n",
        "\n",
        "            pred_boxes = [pred[:4] for pred in preds]\n",
        "            num_total_pred += len(pred_boxes)\n",
        "\n",
        "            # Match prediction with gt bbox by IoU\n",
        "            best_iou = 0\n",
        "            matched_idx = -1\n",
        "            for j, pbox in enumerate(pred_boxes):\n",
        "                iou_score = iou(gt_box, pbox)\n",
        "                if iou_score > best_iou:\n",
        "                    best_iou = iou_score\n",
        "                    matched_idx = j\n",
        "\n",
        "            detection_correct = False\n",
        "            pred_text = \"unreadable\"\n",
        "\n",
        "            if best_iou >= IOU_THRESHOLD:\n",
        "                num_correct_detections += 1\n",
        "                detection_correct = True\n",
        "\n",
        "                px1, py1, px2, py2 = map(int, pred_boxes[matched_idx])\n",
        "                px1, py1, px2, py2 = expand_box(px1, py1, px2, py2, w, h)\n",
        "\n",
        "                plate_crop = img_bgr[py1:py2, px1:px2]\n",
        "                if plate_crop.shape[0] < 32 or plate_crop.shape[1] < 100:\n",
        "                    scale_h = max(32, plate_crop.shape[0])\n",
        "                    scale_w = max(100, plate_crop.shape[1])\n",
        "                    plate_crop = cv2.resize(plate_crop, (scale_w, scale_h))\n",
        "\n",
        "                plate_crop_rgb = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2RGB)\n",
        "                ocr_results = ocr_model.ocr(plate_crop_rgb)\n",
        "\n",
        "                if ocr_results:\n",
        "                    texts = []\n",
        "                    scores = []\n",
        "                    for res_dict in ocr_results:\n",
        "                        texts.extend(res_dict.get('rec_texts', []))\n",
        "                        scores.extend(res_dict.get('rec_scores', []))\n",
        "                    if texts:\n",
        "                        best_idx = np.argmax(scores)\n",
        "                        pred_text = texts[best_idx]\n",
        "                        pred_text_clean = pred_text.replace('·','')\n",
        "                        gt_text_clean = gt_text.replace('·','')\n",
        "                        if editdistance.eval(pred_text_clean, gt_text_clean) <= 1:\n",
        "                            num_correct_ocr += 1\n",
        "\n",
        "            # Visualization\n",
        "            img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "            draw = ImageDraw.Draw(img_pil)\n",
        "\n",
        "            print(f\"gt_text: '{gt_text}'\")\n",
        "\n",
        "            # Draw GT box in green\n",
        "            draw.rectangle(gt_box, outline=\"cyan\", width=2)\n",
        "            #draw.text((gt_box[0], max(0, gt_box[1]-30)), gt_text, font=font, fill=\"cyan\")\n",
        "            img_h, img_w = img_bgr.shape[:2]\n",
        "            gt_text_y = max(0, min(gt_box[1] - 30, img_h - font_size))\n",
        "            draw.text((gt_box[0], gt_text_y), gt_text, font=font, fill=\"cyan\")\n",
        "\n",
        "            # Draw predicted boxes in magenta\n",
        "            for pbox in pred_boxes:\n",
        "                pbox_int = list(map(int, pbox))\n",
        "                draw.rectangle(pbox_int, outline=\"magenta\", width=2)\n",
        "\n",
        "            if detection_correct:\n",
        "                #draw.text((px1, py2 + 5), pred_text_clean, font=font, fill=\"magenta\")\n",
        "                pred_text_y = max(0, min(py2 + 5, img_h - font_size))\n",
        "                draw.text((px1, pred_text_y), pred_text_clean, font=font, fill=\"magenta\")\n",
        "\n",
        "            img_out = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
        "            # Save results\n",
        "            out_path = save_dir / f\"det_{num_total_gt}.jpg\"\n",
        "            cv2.imwrite(str(out_path), img_out)\n",
        "\n",
        "            processed_images += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "# Metrics report\n",
        "precision = num_correct_detections / num_total_pred if num_total_pred > 0 else 0\n",
        "recall = num_correct_detections / num_total_gt if num_total_gt > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "ocr_accuracy = num_correct_ocr / num_correct_detections if num_correct_detections > 0 else 0\n",
        "\n",
        "print(f\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Detection Precision: {precision:.3f}\")\n",
        "print(f\"Detection Recall: {recall:.3f}\")\n",
        "print(f\"Detection F1-score: {f1:.3f}\")\n",
        "print(f\"OCR Accuracy (on detected plates): {ocr_accuracy:.3f}\")\n",
        "\n",
        "print(\"DONE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline evaluation"
      ],
      "metadata": {
        "id": "ieZG_JOOA0aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(box1, box2):\n",
        "    # box1, box2 are tensors or lists: [x1, y1, x2, y2] normalized coords (0-1)\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    if union_area == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return inter_area / union_area\n",
        "\n",
        "def predict_license_plate(image_path, detector_model, recognizer_model, dataset_instance, device, full_image_transform, plate_crop_target_size):\n",
        "    detector_model.eval(), recognizer_model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = full_image_transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        predicted_bbox_batch = detector_model(img_tensor)\n",
        "    predicted_bbox = predicted_bbox_batch[0].clamp(0, 1)\n",
        "    cropped_plates_batch = crop_license_plate(img_tensor, predicted_bbox.unsqueeze(0), target_size=plate_crop_target_size)\n",
        "    cropped_plate_image = cropped_plates_batch[0]\n",
        "    with torch.no_grad():\n",
        "        predictions_list = recognizer_model(cropped_plate_image.unsqueeze(0).to(device))\n",
        "    predicted_indices = [torch.argmax(pred, dim=1).item() for pred in predictions_list]\n",
        "    predicted_string = dataset_instance._indices_to_string(predicted_indices)\n",
        "    return predicted_string, predicted_bbox.cpu(), cropped_plate_image.cpu()\n",
        "\n",
        "def predict_license_plate_yolo(image_path, detector_model, recognizer_model, dataset_instance, device, full_image_transform, plate_crop_target_size):\n",
        "    detector_model.eval()\n",
        "    recognizer_model.eval()\n",
        "\n",
        "    # --- Load and prepare image ---\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "\n",
        "    # --- YOLOv5 Detection ---\n",
        "    results = detector_model(image_path)\n",
        "    detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "    if len(detections) == 0:\n",
        "        # No detections found\n",
        "        empty_bbox = torch.tensor([0, 0, 0, 0], dtype=torch.float32)\n",
        "        return \"\", empty_bbox, None\n",
        "\n",
        "    # Use the detection with highest confidence\n",
        "    best_det = detections[detections[:, 4].argmax()]\n",
        "    x1, y1, x2, y2, conf, cls = best_det\n",
        "    pred_bbox_abs = torch.tensor([x1, y1, x2, y2], dtype=torch.float32)\n",
        "\n",
        "    # --- Normalize bbox (if needed for IoU calc) ---\n",
        "    pred_bbox = torch.tensor([\n",
        "        x1 / width,\n",
        "        y1 / height,\n",
        "        x2 / width,\n",
        "        y2 / height\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    # --- Crop the license plate region ---\n",
        "    transform_plate_crop = A.Compose([\n",
        "        A.Resize(plate_crop_target_size[1], plate_crop_target_size[0]),  # (H, W)\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),  # o usa valori RGB se necessario\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    plate_crop = img.crop((x1, y1, x2, y2)).resize(plate_crop_target_size)\n",
        "    plate_np = np.array(plate_crop)  # Converti PIL -> NumPy\n",
        "    transformed = transform_plate_crop(image=plate_np)\n",
        "    plate_tensor = transformed[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "    # --- Recognition ---\n",
        "    with torch.no_grad():\n",
        "        predictions_list = recognizer_model(plate_tensor)\n",
        "\n",
        "    predicted_indices = [torch.argmax(pred, dim=1).item() for pred in predictions_list]\n",
        "    predicted_string = dataset_instance._indices_to_string(predicted_indices)\n",
        "\n",
        "    return predicted_string, pred_bbox, plate_tensor.cpu()"
      ],
      "metadata": {
        "id": "FJH2NTA3E2yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_YOLO = True\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"\\n\\n--- Starting Full Pipeline Evaluation on Validation Set ---\")\n",
        "\n",
        "# --- Evaluation Parameters ---\n",
        "NUM_EVAL_SAMPLES = 2000\n",
        "IOU_THRESHOLD = 0.7 # A stricter threshold for what counts as a good detection\n",
        "\n",
        "# --- Load models and validation dataset ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if USE_YOLO:\n",
        "  # Load YOLOv5 model\n",
        "  loaded_detector = torch.hub.load('ultralytics/yolov5', 'custom', path=YOLO_WEIGHTS_PATH, source='github')\n",
        "  loaded_detector.conf = 0.25  # adjust confidence threshold if needed\n",
        "\n",
        "else:\n",
        "  # Load Detector Model\n",
        "  loaded_detector = LicensePlateDetector().to(device)\n",
        "  loaded_detector.load_state_dict(torch.load(DETECTOR_MODEL_PATH, map_location=device))\n",
        "  loaded_detector.eval()\n",
        "\n",
        "\n",
        "# Create a validation dataset instance for evaluation\n",
        "val_dataset_instance = CCPDDataset(data_dir=VAL_DATASET, transform=transform_val)\n",
        "\n",
        "# Load Recognizer Model\n",
        "loaded_recognizer = LicensePlateRecognizer(\n",
        "    num_provinces=len(val_dataset_instance.provinces),\n",
        "    num_alphabets=len(val_dataset_instance.alphabets),\n",
        "    num_ads=len(val_dataset_instance.ads)\n",
        ").to(device)\n",
        "loaded_recognizer.load_state_dict(torch.load(RECOGNITION_MODEL_PATH, map_location=device))\n",
        "loaded_recognizer.eval()\n",
        "\n",
        "print(f\"Models loaded. Evaluating on {NUM_EVAL_SAMPLES} samples from the validation set...\")\n",
        "\n",
        "# --- Initialize metric counters ---\n",
        "iou_scores = []\n",
        "correct_detections = 0\n",
        "correct_recognitions = 0\n",
        "total_chars = 0\n",
        "correct_chars = 0\n",
        "e2e_correct_matches = 0\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "# This loop iterates through the specified number of validation samples\n",
        "for i in tqdm(range(NUM_EVAL_SAMPLES), desc=\"Evaluating Pipeline\"):\n",
        "    # Ensure we don't go out of bounds of the dataset\n",
        "    if i >= len(val_dataset_instance):\n",
        "        print(f\"\\nWarning: Requested {NUM_EVAL_SAMPLES} samples, but validation set only has {len(val_dataset_instance)}. Stopping.\")\n",
        "        NUM_EVAL_SAMPLES = i\n",
        "        break\n",
        "\n",
        "    # Get ground truth data from the dataset\n",
        "    _, gt_bbox, gt_string, _ = val_dataset_instance[i]\n",
        "    sample_image_path = val_dataset_instance.image_files[i]\n",
        "\n",
        "    # Run the full end-to-end pipeline to get predictions\n",
        "    if USE_YOLO:\n",
        "      pred_string, pred_bbox, _ = predict_license_plate_yolo(\n",
        "        image_path=sample_image_path,\n",
        "        detector_model=loaded_detector,\n",
        "        recognizer_model=loaded_recognizer,\n",
        "        dataset_instance=val_dataset_instance,\n",
        "        device=device,\n",
        "        full_image_transform=transform_val,\n",
        "        plate_crop_target_size=PLATE_CROP_TARGET_SIZE\n",
        "    )\n",
        "    else:\n",
        "      pred_string, pred_bbox, _ = predict_license_plate(\n",
        "          image_path=sample_image_path,\n",
        "          detector_model=loaded_detector,\n",
        "          recognizer_model=loaded_recognizer,\n",
        "          dataset_instance=val_dataset_instance,\n",
        "          device=device,\n",
        "          full_image_transform=transform_val,\n",
        "          plate_crop_target_size=PLATE_CROP_TARGET_SIZE\n",
        "      )\n",
        "\n",
        "    # --- Calculate and Aggregate Metrics for this sample ---\n",
        "\n",
        "    # 1. Detection Metrics\n",
        "    iou = compute_iou(pred_bbox.cpu().numpy(), gt_bbox.cpu().numpy())\n",
        "    iou_scores.append(iou)\n",
        "    is_detection_correct = iou >= IOU_THRESHOLD\n",
        "    if is_detection_correct:\n",
        "        correct_detections += 1\n",
        "\n",
        "    # 2. Recognition Metrics\n",
        "    is_recognition_correct = (pred_string == gt_string)\n",
        "    if is_recognition_correct:\n",
        "        correct_recognitions += 1\n",
        "\n",
        "    # Character Recognition Rate (CRR)\n",
        "    for j in range(len(gt_string)):\n",
        "        if j < len(pred_string) and pred_string[j] == gt_string[j]:\n",
        "            correct_chars += 1\n",
        "    total_chars += len(gt_string)\n",
        "\n",
        "    # 3. End-to-End Metric\n",
        "    if is_detection_correct and is_recognition_correct:\n",
        "        e2e_correct_matches += 1\n",
        "\n",
        "# --- Calculate Final Averages and Accuracies ---\n",
        "# This happens after the loop has processed all samples\n",
        "avg_iou = np.mean(iou_scores)\n",
        "detection_accuracy = correct_detections / NUM_EVAL_SAMPLES if NUM_EVAL_SAMPLES > 0 else 0\n",
        "recognition_accuracy = correct_recognitions / NUM_EVAL_SAMPLES if NUM_EVAL_SAMPLES > 0 else 0\n",
        "character_recognition_rate = correct_chars / total_chars if total_chars > 0 else 0\n",
        "e2e_accuracy = e2e_correct_matches / NUM_EVAL_SAMPLES if NUM_EVAL_SAMPLES > 0 else 0\n",
        "\n",
        "# --- Print Final Results Summary ---\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "print(\"--- FULL PIPELINE EVALUATION RESULTS ---\")\n",
        "print(\"=\"*45)\n",
        "print(f\"Evaluated on: {NUM_EVAL_SAMPLES} samples\")\n",
        "print(\"-\" * 45)\n",
        "print(\"DETECTION METRICS:\")\n",
        "print(f\"  - Average IoU: {avg_iou:.4f}\")\n",
        "print(f\"  - Detection Accuracy (IoU >= {IOU_THRESHOLD}): {detection_accuracy * 100:.2f}%\")\n",
        "print(\"-\" * 45)\n",
        "print(\"RECOGNITION METRICS:\")\n",
        "print(f\"  - Exact Match Accuracy (Full Plate): {recognition_accuracy * 100:.2f}%\")\n",
        "print(f\"  - Character Recognition Rate (CRR): {character_recognition_rate * 100:.2f}%\")\n",
        "print(\"-\" * 45)\n",
        "print(\"END-TO-END (E2E) METRICS:\")\n",
        "print(f\"  - E2E Accuracy (Correct Detection & Recognition): {e2e_accuracy * 100:.2f}%\")\n",
        "print(\"=\"*45)"
      ],
      "metadata": {
        "id": "LYzsWotkA6z0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "egbAiWbgfGmf",
        "K2tgLoHCxYtU",
        "0WGopU37nvNq",
        "U380SvxItSGw",
        "7laHBSKiuwr4",
        "t1dtKO735sJf",
        "5OLCbAe3sOxz",
        "7k22MEntnXtV",
        "afjshDtonnrF",
        "FMd_8OR9xoYt",
        "2TNROD4AyCSE",
        "iCmaHDxJyWXS"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cd237cb105f49cbaab7ba135811d72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_939356461876484294f0606676ade952",
              "IPY_MODEL_c62d6a9d4989473282342e0e2e8af930",
              "IPY_MODEL_4aef59bdcc894b8987bf237386f59342"
            ],
            "layout": "IPY_MODEL_339ed757b6c743ef9f2e90cc65c8368e"
          }
        },
        "939356461876484294f0606676ade952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241aa8c95bf9463ebd566a26474898bc",
            "placeholder": "​",
            "style": "IPY_MODEL_ec599cc444c745379ca8abcc284eeedd",
            "value": "Fetching 6 files: 100%"
          }
        },
        "c62d6a9d4989473282342e0e2e8af930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_095aa5e7de034229bcd10e86bead36cf",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8ba623cbc3452ea380064d9372344c",
            "value": 6
          }
        },
        "4aef59bdcc894b8987bf237386f59342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14c6e71c6f541c3a1c97ec7236d1fe8",
            "placeholder": "​",
            "style": "IPY_MODEL_653de4eca68c4f03ae228f1c28b9c2cc",
            "value": " 6/6 [00:00&lt;00:00, 399.44it/s]"
          }
        },
        "339ed757b6c743ef9f2e90cc65c8368e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241aa8c95bf9463ebd566a26474898bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec599cc444c745379ca8abcc284eeedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "095aa5e7de034229bcd10e86bead36cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8ba623cbc3452ea380064d9372344c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c14c6e71c6f541c3a1c97ec7236d1fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "653de4eca68c4f03ae228f1c28b9c2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72759df25bb74d68bfb9faca58f3ec6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58227331dcd64728bf8c8f0ba007fcd4",
              "IPY_MODEL_07e7f16d9b6c4b3bb1704d685cf43731",
              "IPY_MODEL_e8d683eb100d46b682f2859b75117317"
            ],
            "layout": "IPY_MODEL_bf545748d8164663a665dfed8229d23d"
          }
        },
        "58227331dcd64728bf8c8f0ba007fcd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ba52c30155481c8393c8f75be71cf7",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e8b5f9265045549bfc79086502fbe1",
            "value": "Fetching 6 files: 100%"
          }
        },
        "07e7f16d9b6c4b3bb1704d685cf43731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0edae4b7118245ac9dc1414d6c261567",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d96563d0fd154d7b8dd4741daa246a12",
            "value": 6
          }
        },
        "e8d683eb100d46b682f2859b75117317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a4f911ede8243ad989e277e2f55a5ba",
            "placeholder": "​",
            "style": "IPY_MODEL_d468a0d7a79346ff80d32bda949a2768",
            "value": " 6/6 [00:00&lt;00:00, 484.88it/s]"
          }
        },
        "bf545748d8164663a665dfed8229d23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ba52c30155481c8393c8f75be71cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e8b5f9265045549bfc79086502fbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0edae4b7118245ac9dc1414d6c261567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96563d0fd154d7b8dd4741daa246a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a4f911ede8243ad989e277e2f55a5ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d468a0d7a79346ff80d32bda949a2768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b75f089a807f4778a71527c4634c668b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18b3618fdceb40ae9b51e6153f9d1ab7",
              "IPY_MODEL_ae0af59a2ee24028b9ed851936276b92",
              "IPY_MODEL_1329008df3ff41f7abbefc2430c67d1f"
            ],
            "layout": "IPY_MODEL_e717af84c26c44ffaf20152eb98691a5"
          }
        },
        "18b3618fdceb40ae9b51e6153f9d1ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d5f9ac50244c95b84260a3928bd37f",
            "placeholder": "​",
            "style": "IPY_MODEL_d3a42d2abd1b46128765caaa9b9fae89",
            "value": "Fetching 8 files: 100%"
          }
        },
        "ae0af59a2ee24028b9ed851936276b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16c65936a5f441558c876bf8c2b5a9b0",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16f195f769a346278b5a888cfe3884fd",
            "value": 8
          }
        },
        "1329008df3ff41f7abbefc2430c67d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95339dc5e0bc44eda8e91572ac975c68",
            "placeholder": "​",
            "style": "IPY_MODEL_e2e7e1175ee74208b921f07270b1c1ae",
            "value": " 8/8 [00:00&lt;00:00, 583.03it/s]"
          }
        },
        "e717af84c26c44ffaf20152eb98691a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d5f9ac50244c95b84260a3928bd37f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3a42d2abd1b46128765caaa9b9fae89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16c65936a5f441558c876bf8c2b5a9b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f195f769a346278b5a888cfe3884fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95339dc5e0bc44eda8e91572ac975c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e7e1175ee74208b921f07270b1c1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94fe8170c8654810aa64d3a8808e9a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6af51f4af14241f09c9c3061a32504d6",
              "IPY_MODEL_4b656117ba834a1292a70f5f65def991",
              "IPY_MODEL_2f9a34b0696e40f28b239a8cb9b437c0"
            ],
            "layout": "IPY_MODEL_05b2ee9401424103a01a9f51c9377c96"
          }
        },
        "6af51f4af14241f09c9c3061a32504d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc52c77254874d89879b04132c4b6364",
            "placeholder": "​",
            "style": "IPY_MODEL_8502fd3f4d4d4990b551e6e824691193",
            "value": "Fetching 6 files: 100%"
          }
        },
        "4b656117ba834a1292a70f5f65def991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1457e1d155b4064a3455eef3cd8d27a",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9000d526bd06499cb4f333b20b584a55",
            "value": 6
          }
        },
        "2f9a34b0696e40f28b239a8cb9b437c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21c5d8fd13cd4f55924bf5f634c996a9",
            "placeholder": "​",
            "style": "IPY_MODEL_88710e821aab4755971e630bdc311ba9",
            "value": " 6/6 [00:00&lt;00:00, 482.01it/s]"
          }
        },
        "05b2ee9401424103a01a9f51c9377c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc52c77254874d89879b04132c4b6364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8502fd3f4d4d4990b551e6e824691193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1457e1d155b4064a3455eef3cd8d27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9000d526bd06499cb4f333b20b584a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21c5d8fd13cd4f55924bf5f634c996a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88710e821aab4755971e630bdc311ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89efe886a59e4b8580064fda9e2b8201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d082f40bdc2f40f79df7686c720b7039",
              "IPY_MODEL_8ad3244beca64eafb133401aae79ff78",
              "IPY_MODEL_b23bd7ba6b36494497b180bd4fe64d4b"
            ],
            "layout": "IPY_MODEL_c36e24c69bad4894bf2b5b92468bb16f"
          }
        },
        "d082f40bdc2f40f79df7686c720b7039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223d9d015d1f41539cbac0313a46d4f3",
            "placeholder": "​",
            "style": "IPY_MODEL_4df8af3ad1ba46619f33b307149aa2e9",
            "value": "Fetching 6 files: 100%"
          }
        },
        "8ad3244beca64eafb133401aae79ff78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5c3e24183ce44569c1dc9039e409111",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b42c2b914b4549a6c7c904ec720f3d",
            "value": 6
          }
        },
        "b23bd7ba6b36494497b180bd4fe64d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_470ab6eb5dc845dfa9af39420fe8e473",
            "placeholder": "​",
            "style": "IPY_MODEL_74b07e19996640cdbb1583c48df4cfc3",
            "value": " 6/6 [00:00&lt;00:00, 321.95it/s]"
          }
        },
        "c36e24c69bad4894bf2b5b92468bb16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223d9d015d1f41539cbac0313a46d4f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4df8af3ad1ba46619f33b307149aa2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5c3e24183ce44569c1dc9039e409111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b42c2b914b4549a6c7c904ec720f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "470ab6eb5dc845dfa9af39420fe8e473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b07e19996640cdbb1583c48df4cfc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}